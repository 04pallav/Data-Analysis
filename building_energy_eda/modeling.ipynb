{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from dateutil import parser\n",
    "from tpot import TPOTRegressor\n",
    "import tensorflow as tf\n",
    "import feather\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   53.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   25.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   52.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Directory with feather files\n",
    "root_dir = 'feather/train_test/'\n",
    "building_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n",
    "\n",
    "# Create the three models with minimal hyperparameter selection\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = 40, criterion = 'mse', n_jobs = -1, verbose = 1)\n",
    "lr = LinearRegression()\n",
    "svr = SVR(kernel = 'rbf', verbose = True)\n",
    "\n",
    "# Need to store RF feature importances\n",
    "feature_importances = pd.DataFrame(columns = building_names)\n",
    "\n",
    "# Iterate through all the buildings\n",
    "for building in building_names:\n",
    "    \n",
    "    # Create a dataframe to hold the predictions\n",
    "    predictions = pd.DataFrame(columns = ['rf', 'lr', 'svr'])\n",
    "    \n",
    "    # Read in the training/testing sets\n",
    "    X_train = np.array(feather.read_dataframe(root_dir + '%s_X_train.feather' % building))\n",
    "    X_test = np.array(feather.read_dataframe(root_dir + '%s_X_test.feather' % building))\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    \n",
    "    # Fit (train) the models and make predictions on the test data\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # Save features importances of the random forest\n",
    "    building_feature_importances = rf.feature_importances_\n",
    "    feature_importances[building] = building_feature_importances\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_predictions = lr.predict(X_test)\n",
    "    \n",
    "    svr.fit(X_train, y_train)\n",
    "    svr_predictions = svr.predict(X_test)\n",
    "    \n",
    "    # Save the predictions to a dataframe\n",
    "    predictions['rf'] = rf_predictions\n",
    "    predictions['lr'] = lr_predictions\n",
    "    predictions['svr'] = svr_predictions\n",
    "    \n",
    "    # Write the predictions \n",
    "    feather.write_dataframe(predictions, 'feather/predictions/' + '%s_predictions.feather' % building)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the feature importances to a feather file for analysis\n",
    "feather.write_dataframe(feature_importances, 'feather/feature_importances/feature_importances.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Random Forest Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform grid search to find best parameters\n",
    "# Feature selection, depth, number of trees, and minimum size of leaves\n",
    "param_grid = {\n",
    "    'max_features': [None, 0.5], \n",
    "    'max_depth': [None, 40],\n",
    "    'n_estimators':[200, 400],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}\n",
    "\n",
    "# Testing on a dataset with a high mape to improve performance\n",
    "nve_X_train = feather.read_dataframe('feather/train_test/NVE_X_train.feather')\n",
    "nve_X_train = np.array(nve_X_train)\n",
    "\n",
    "nve_X_test = feather.read_dataframe('feather/train_test/NVE_X_test.feather')\n",
    "nve_X_test = np.array(nve_X_test)\n",
    "\n",
    "nve_y_train = feather.read_dataframe('feather/train_test/NVE_y_train.feather')\n",
    "nve_y_train = np.array(nve_y_train['forecast'])\n",
    "\n",
    "nve_y_test = feather.read_dataframe('feather/train_test/NVE_y_test.feather')\n",
    "nve_y_test = np.array(nve_y_test['forecast'])\n",
    "\n",
    "rf_test = RandomForestRegressor(bootstrap = True, verbose = 2, n_jobs = -1)\n",
    "\n",
    "rf_grid = GridSearchCV(rf_test, param_grid, scoring = 'neg_mean_squared_error', n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 40.4min\n"
     ]
    }
   ],
   "source": [
    "rf_grid.fit(nve_X_train, nve_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 0.5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Best Grid Search Parameters to Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   42.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   39.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   26.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   11.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   40.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   25.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Implement best grid search results\n",
    "\n",
    "# Directory with feather files\n",
    "root_dir = 'feather/train_test/'\n",
    "building_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n",
    "\n",
    "# Create the three models with optimized hyperparameters\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = None, min_samples_leaf = 1, max_features = 0.5,\n",
    "                           criterion = 'mse', n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Need to store RF feature importances\n",
    "feature_importances = pd.DataFrame(columns = building_names)\n",
    "\n",
    "# Iterate through all the buildings\n",
    "for building in building_names:\n",
    "    \n",
    "    # Create a dataframe to hold the predictions\n",
    "    predictions = pd.DataFrame(columns = ['rf'])\n",
    "    \n",
    "    # Read in the training/testing sets\n",
    "    X_train = np.array(feather.read_dataframe(root_dir + '%s_X_train.feather' % building))\n",
    "    X_test = np.array(feather.read_dataframe(root_dir + '%s_X_test.feather' % building))\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    \n",
    "    # Fit (train) the models and make predictions on the test data\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # Save features importances of the random forest\n",
    "    building_feature_importances = rf.feature_importances_\n",
    "    feature_importances[building] = building_feature_importances\n",
    "    \n",
    "    \n",
    "    # Save the predictions to a dataframe\n",
    "    predictions['rf'] = rf_predictions\n",
    "    \n",
    "    # Write the predictions \n",
    "    feather.write_dataframe(predictions, 'feather/predictions/' + '%s_optimized_predictions.feather' % building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the feature importances to a feather file for analysis\n",
    "feather.write_dataframe(feature_importances, 'feather/feature_importances/optimized_feature_importances.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Model\n",
    "\n",
    "Predict last six months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   52.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   44.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoServ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   39.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGE1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   49.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMUD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   25.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRP\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'feather/challenge_predictions/'\n",
    "building_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n",
    "\n",
    "                  \n",
    "# Create the three models with minimal hyperparameter selection\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = None, min_samples_leaf = 1, max_features = 0.5,\n",
    "                           criterion = 'mse', n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Need to store RF feature importances\n",
    "feature_importances = {}\n",
    "\n",
    "# Iterate through all the buildings\n",
    "for building in building_names:\n",
    "    \n",
    "    # Create a dataframe to hold the predictions\n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    # Read in the training/testing sets\n",
    "    X_train = np.array(feather.read_dataframe(root_dir + '%s_X_train.feather' % building))\n",
    "    X_test = np.array(feather.read_dataframe(root_dir + '%s_X_test.feather' % building))\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['forecast'])\n",
    "    \n",
    "    \n",
    "    # Fit (train) the models and make predictions on the test data\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # Save features importances of the random forest\n",
    "    building_feature_importances = rf.feature_importances_\n",
    "    feature_importances[building] = building_feature_importances\n",
    "    \n",
    "    # Save the predictions to a dataframe\n",
    "    predictions['predictions'] = rf_predictions\n",
    "    \n",
    "    # Write the predictions to a feather file for moving back to r for analysis\n",
    "    feather.write_dataframe(predictions, 'feather/challenge_results/%s_preds.feather' % building)\n",
    "    print(building)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Last 4 Months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Energy Consumption (Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoServ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   25.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 15.4min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGE1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   39.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMUD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   57.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRP\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'feather/months/'\n",
    "building_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n",
    "\n",
    "                  \n",
    "# Create the three models with minimal hyperparameter selection\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = None, min_samples_leaf = 1, max_features = 0.5,\n",
    "                           criterion = 'mse', n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Need to store RF feature importances\n",
    "feature_importances = pd.DataFrame(columns = building_names)\n",
    "\n",
    "# Iterate through all the buildings\n",
    "for building in building_names:\n",
    "    \n",
    "    # Create a dataframe to hold the predictions\n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    # Read in the training/testing sets\n",
    "    X_train = np.array(feather.read_dataframe(root_dir + '%s_X_train.feather' % building))\n",
    "    X_test = np.array(feather.read_dataframe(root_dir + '%s_X_test.feather' % building))\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['cleaned_energy'])\n",
    "    y_train = np.array(feather.read_dataframe(root_dir + '%s_y_train.feather' % building)['cleaned_energy'])\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit (train) the models and make predictions on the test data\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_predictions = rf.predict(X_test)\n",
    "    \n",
    "    # Save features importances of the random forest\n",
    "    building_feature_importances = rf.feature_importances_\n",
    "    feature_importances[building] = building_feature_importances\n",
    "    \n",
    "    # Save the predictions to a dataframe\n",
    "    predictions['predictions'] = rf_predictions\n",
    "    \n",
    "    # Write the predictions to a feather file for moving back to r for analysis\n",
    "    feather.write_dataframe(predictions, 'feather/predictions/%s_month_predictions.feather' % building)\n",
    "    print(building)\n",
    "    \n",
    "# Write the feature importances to a feather file for analysis\n",
    "feather.write_dataframe(feature_importances, 'feather/feature_importances/month_feature_importances.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increased Temp Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.9min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoServ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   49.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   42.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGE1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   53.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMUD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRP\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'feather/increased/'\n",
    "building_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n",
    "   \n",
    "# Create the three models with optimized hyperparameter selection\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = None, min_samples_leaf = 1, max_features = 0.5,\n",
    "                           criterion = 'mse', n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Need to store RF feature importances\n",
    "feature_importances = pd.DataFrame(columns = building_names)\n",
    "\n",
    "# Iterate through all the buildings\n",
    "for building in building_names:\n",
    "    \n",
    "    # Create a dataframe to hold the predictions\n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    # Read in the training/testing sets\n",
    "    train = np.array(feather.read_dataframe(root_dir + '%s_train.feather' % building))\n",
    "    labels = np.array(feather.read_dataframe(root_dir + '%s_labels.feather' % building)['forecast'])\n",
    "    test = np.array(feather.read_dataframe(root_dir + '%s_test.feather' % building))\n",
    "    \n",
    "    # Scale the data \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Scaler is fit on train data and run on testing data\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    \n",
    "    # Fit (train) the models and make predictions on the test data\n",
    "    rf.fit(train, labels)\n",
    "    rf_predictions = rf.predict(test)\n",
    "    \n",
    "    # Save features importances of the random forest\n",
    "    building_feature_importances = rf.feature_importances_\n",
    "    feature_importances[building] = building_feature_importances\n",
    "    \n",
    "    # Save the predictions to a dataframe\n",
    "    predictions['predictions'] = rf_predictions\n",
    "    \n",
    "    # Write the predictions to a feather file for moving back to r for analysis\n",
    "    feather.write_dataframe(predictions, 'feather/predictions/%s_increased_predictions.feather' % building)\n",
    "    print(building)\n",
    "    \n",
    "# Write the feature importances to a feather file for analysis\n",
    "feather.write_dataframe(feature_importances, 'feather/feature_importances/increased_feature_importances.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing One Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=50,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smud_X_train = np.array(feather.read_dataframe('feather/increased/SMUD_train.feather'))\n",
    "smud_y_train = np.array(feather.read_dataframe('feather/increased/SMUD_labels.feather')['forecast'])\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 10, max_depth = 3, verbose = 2, min_samples_split = 50)\n",
    "\n",
    "rf.fit(smud_X_train, smud_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_tree = rf.estimators_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vis_tree\n",
    "smud_features = list(feather.read_dataframe('feather/increased/SMUD_train.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(vis_tree, out_file = 'treevis.dot', feature_names = smud_features, rounded = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('treevis.dot')\n",
    "graph.write_png('treevis_verysmall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import check_call\n",
    "check_call(['dot','-Tpng','treevis.dot','-o','treevis.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = None, min_samples_leaf = 1, max_features = 0.5,\n",
    "                           criterion = 'mse', n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.5, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
