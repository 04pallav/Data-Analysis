{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Progressive Building Energy EDA: Report Three'\nauthor: \"William Koehrsen\"\ndate: \"December 5, 2017\"\noutput:\n  pdf_document:\n    toc: true\n    number_sections: true\nurlcolor: blue\n---\n\n<!--\n  # Script Name: eda_report_three.Rmd\n  # Purpose: Third report for DSCI 451 Semester Project\n  # Authors: William Koehrsen\n  # License: Creative Commons Attribution-ShareAlike 4.0 International License.\n  ##########\n# Latest Changelog Entries:\n# v0.00.01 - 11/20/17 - eda_report_three.Rmd - William Koehrsen began this rmd\n# v0.00.02 - 11/22/17 - eda_report_three.Rmd - William Koehrsen completed\n#  types of modeling\n# v0.00.03 - 11/25/17 - eda_report_three.Rmd - William Koehrsen completed model\n# selection\n# v0.00.04 - 11/28/17 - eda_report_three.Rmd - William Koehrsen completed \n# model validation and intrepretation\n# v0.00.05 - 11/30/17 - eda_report_three.Rmd - William Koehrsen completed \n# model challenge and conclusions\n# v0.00.06 - 12/02/17 - eda_report_three.Rmd - William Koehrsen completed first draft\n# v0.00.07 - 12/04/17 - eda_report_three.Rmd - William Koehrsen completed first edit\n# v0.00.08 - 12/05/17 - eda_report_three.Rmd - William Koehrsen made final adjustments to report\n##########\n\n# Rmd code goes below the comment marker!\n-->\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(root_dir = 'C:/Users/Will Koehrsen/Documents/DSCI 451/17f-dsci351-451-wjk68/1-assignments/SemProj-451/building_energy_eda/scripts')\nknitr::opts_chunk$set(cache = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\n\n# Load in libraries required for anaysis\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(stats)\nlibrary(tidyverse); theme_set(theme_classic(12)) + theme(axis.text = element_text(color = 'black'), plot.title = element_text(hjust = 0.5))\nlibrary(data.table)\nlibrary(car)\nlibrary(dummies)\nlibrary(randomForest)\nlibrary(caret)\nlibrary(feather)\nlibrary(Metrics)\n```\n\n```{r, echo = FALSE}\n# Helper function to read in a building energy dataframe properly\nread_data <- function(filename) {\n  df <- as.data.frame(suppressMessages(read_csv(paste0('../data/', filename))))\n  df$timestamp <- as.POSIXct(df$timestamp)\n  df$day_of_week <- as.factor(df$day_of_week)\n  df$week_day_end <- as.factor(df$week_day_end)\n  df$sun_rise_set <- as.factor(df$sun_rise_set)\n  return(df)\n}\n\n# Example\ndf <- read_data('../data/f-SRP_weather.csv')\n\n# List of files names\nfilenames <- dir('../data/')\n\n# Axis labels theme for plots\nth <- theme(axis.text = element_text(face = 'bold', color = 'black', size = 12))\n```\n# Introduction \n\n\n__The overall guiding question for this project is what explanatory variables are__\n__most highly correlated with energy consumption, and is it possible to build a__\n__model to predict energy consumption from the explanatory variables?__\n\nThe first report introduced the background for the Progressive Building Energy \nExploratory Data Analysis (EDA) and the purpose of the EDIFES project. Several questions were posed in the \nfirst report to guide the EDA outlined in Report Two.  Report To detailed my initial forays into the data in which I identified trends \nbetween weather and energy consumption and the relationship between the square\nfootage of a building and the annual energy consumption. I also looked at pairwise\nplots to find the strongest relationships between variables, performed a power transformation using\nthe Box-Cox transformation, and evaluated three different models for accuracy\nin predicting the energy consumption from the weather and time variables. \n\nReport Three will focus on an in-depth modeling investigation with an emphasis on\ninterpretability and accuracy. Several types of modeling are documented with \nvarying degrees of complexity from linear regression models to a support vector machine with \namounts of interpretability. This report expands on the exploratory work done in\nreport two and aims to provide models and conclusions that can be used by the EDIFES\nteam for performing virtual building energy audits. Modeling allows us not only to \nexplain the past based on the observations, but to predict future behavior so that we may adjust our actions accordingly. \n\n# Types of Modeling\n\nThe challenge of predicting energy consumption from weather and time data is supervised because \nwe know the actual answers, which in this case are the values of energy at each fifteen minute interval.\nThere are an almost unlimited number of modeling approaches for regression tasks that can be implemented in R and Python. \nThese range in complexity from linear models with simple equations and \nunderstandable variable weights, to deep neural networks which are essentially black boxes \nthat accept data as an input, perform a sequence\nof mathematical operations, and produce an (often correct) answer with no justification. For this \nproject, I will evaluate three different modeling approaches, covering the spectrum of complexity:\nLinear Regression, a Random Forest Regressor, and a a Support Vector Machine Regressor. \n\n\n## Supervised Learning\n\nI am approaching this modeling problem as a supervised regression task. We are given a \ndependent response variable, energy use, and asked to use the explanatory (theoretically independent) variables\nto explain and predict the response. If I had thousands of buildings, clustering could \nbe an interesting approach to find similarities between buildings in climate zones,\nor maybe to find groups of buildings that behave similarly, but with only \neight buildings, the possibilities for clustering or unsupervised learning\nprocesses are limited. Another option for unsupervised learning is feature \nreduction/selection with [principal component analysis (PCA)](https://onlinecourses.science.psu.edu/stat505/node/51), \nbut when this is performed, the variables lose their physical representation and the \nentire process then is unintrepretable to humans. Primarily for the reason of \nintrepretability, I will limit this report to supervised learning, and the task\nof predicting energy usage from the weather and time variables. \n\n\n## Single and Multivariate Linear Regression\n\nThe simplest model and the ideal place to start  is linear regression which aims to explain the variance\nin the response variable by a weighted linear addition of the explanatory variables. Linear Regression\ncan use a single explanatory (independent) variable, or it can use many. Taking a power \ntransformation of the explanatory variables in order to explain the dependent variable\nis still a linear model because the response model is still a linear addition of coefficients multiplied\nby the respective independent variable. The general equation for a linear model is\n$$y = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n$$\nwhere $w_0$ is the intercept, $x_n$ represents the explanatory variables, $w_n$ is the weight \nassigned to each explanatory variable,\nand $y$ is the response variable. In the case of the Building Energy Dataset,\ny is the energy consumption, x is the weather or time features, and w is the weight\nassigned to each feature. The weight in a linear model represents the slope of the relationship, or \nhow much a change in the x variable affects the \ny variable. A great aspect of linear modeling is\nthat the impact of change in one independent variable can be directly observed and predicted.\n\nAs a reminder, here is a summer and winter correlation heatmap showing the \ncorrelation coefficients between the weather variables and the energy consumption\nfor each building. These relationships can be used to build a linear model \nby using only the strongest correlations. \n\n```{r, echo = FALSE}\nknitr::include_graphics('../images/Summer_corr_heatmap.png')\n```\n\n```{r, echo = FALSE}\nknitr::include_graphics('../images/Winter_corr_heatmap.png')\n\n```\n\nThis shows the most highly correlated variables with energy consumption which\nmight be a good place to start for creating linear models. At this point, it\nmakes sense to segment by season because the coefficients vary greatly \nbetween the winter and the summer. The objective with linear modeling is to create a parsimonious model,\nwhich can be thought of as the simplest model with the highest level of performance. Generally in regression,\nthis means having as few \nexplanatory variables as possible while minimizing the sum of squared errors. \n\n## Random Forest\n\nTo understand the powerful random forest, you first need to grasp the concept of a decision tree.\nThe best way to describe a single decision tree is as a flowchart of questions about the variable values\nof an observation\nthat leads in a classification/prediction. Each question (known as a node) has a yes/no answer based on the value of a particular variable.\nThe two answer form branches leading away from the node. Eventually, \nthe tree terminates in the final classification/prediction node called a leaf. \nA single decision tree can be arbitrarily large and deep depending on the number of features\nand the number of classes. They are adept at both classification and regression and \ncan learn a non-linear decision boundary (they actually learn many small linear \ndecision boundaries which collectively are non-linear). However, a single decision tree\nis very prone to overfitting, especially as the depth increases. The decision tree is flexible \nleading to a tendency to simply memorize the training data. To solve this problem, \nensembles of decision trees are combined into a powerful classifier known \nas a random forest. Each tree in the forest is trained on a randomly chosen subset of the \ntraining data (either with replacement, called bootstrapping, or without) and \n on a subset of the features. This increases variability between trees making\nthe overall forest more robust and less prone to overfitting. In order to make predictions,\nthe random forest passes the features (values of variables) of the observation to all trees, and takes an average \nof the votes of each tree (known as bagging). The random forest can also weight the votes \nof each tree with respect to the confidence the tree has in its prediction. Overall, \nthe random forest is fast, relatively simple, has a moderate level of interpretability,\nand performs extremely well on both classification and regression tasks. The random\nforest should be one of the first models tried on any machine learning problem and is generally my second approach after a linear model.\nThere are a number of hyperparameters that must be specified for the forest ahead of time\nwith the most important the number of trees in the forest, the number of features \nconsidered by each tree, the depth of the tree, and the minimum number of observations\npermitted at each leaf of the tree. These can be selected by training many different models\nwith varying hyperparameters and selecting the combination that performs \nbest on cross-validation or a testing set. A random forest performs implicit feature \nselection and can return the relative importances of the features so it can \nbe used as a method to reduce dimensions for additional algorithms.\n\nA typical model of a single decision tree is presented below:\n\n```{r, echo = FALSE}\nknitr::include_graphics('../decision_tree_loan.png')\n\n```\n\nThis particular decision tree is used to make predictions about whether or not\nan individual will default on a loan based on numerous personal features. Much as a single analyst might make a bad judgement often but \nan entire roomful will typically make the right decision, a forest of decision trees \nis much better equipped to make correct predictions \nthan even the most capable individual decision tree.\n\n\n## SVM\n\nA support vector machine (SVM) regressor is the most complicated and the least \ninterpretable of the models explored in this report. SVM can be used for\nboth classification and regression and operate on the basis of finding\na hyperplane to separate classes. The idea is that any decision boundary becomes linear\nin a high-dimensional space. For example, \na linear decision boundary in three-dimensional space is a plane. \nThe SVM projects the features of the \nobservations into a higher dimensional space using a kernel, which is\nsimply a transformation of the data. The model then finds the plane that best \nseparates the data by maximizing the margin, the minimum distance\nbetween the nearest member of each class and the decision boundary. The support vectors in the name\nof the algorithm refer to the points closest to the decision boundary which are\nrefereed to as the support and have the greatest influence on the position of the \nhyperplane. SVM regressors work by fitting a non-parametric regression model to the \ndata and trying to minimize the distance between the model and all training instances. \nSVM models are more complex tan either a linear regressor or a random forest regressor\nand hence have almost no interpretability. The transformation of the features into \na higher-dimensional space using a kernel removes all physical representation \nof the features. SVM models are truly black boxes, but have high accuracy on \nsmall datasets with limited amounts of noise. The support vector machine also \ntakes much longer to train than either of the other models. I primarily am using this \nalgorithm to evaluate an additional approach to cover the spectrum of models and because it showed \npromising results in the literature. \n\nExamples of the classification ability of SVM using different kernels is shown below:\n\n```{r, echo = FALSE}\nknitr::include_graphics('../svc_kernel.png')\n```\n\nThe RBF or Radial Basis Kernel, is the most popular kernel in use\nand is the default in many implementations. I have little experience with SVMs, and hence will\nrely mainly on the default specifications of the hyperparamters when evaluating the model. \n\n# Statistical Prediction/Modeling\n\nThe EDIFEES project is primarily concerned with reducing the energy consumption\nof buildings and theretofore, a sensible place to begin modeling is trying to predict the daily\nelectricity use of a building. The only information available for this prediction is the past \nelectricity consumption and weather variables. The problem is a supervised regression learning\ntask because we have known targets, the energy consumption, that we wish to predict\nform the independent variables, the time and weather information. Once we\nare able to predict how much electricity a building will use on a given day, the next step\nwill be to find the main drivers of that electricity usage and see if they can \nbe mitigated or prepared for. I will start with daily prediction because any longer\nperiod of time will be too coarse to predict with accuracy. Moreover, aggregating\nacross periods of time essentially reduces the information value because the inherent \nvariability is smoothed out. This smoothing could make it more difficult to predict the \nvariance in the response variables from the variance in the independent variables. \nMy approach will treat each building separately because I was not able to determine\na satisfactory method for normalizing energy consumption between buildings of\ndifferent sizes in varying climate zones. This should not be a problem, because \nEDIFES will theoretically always have access to the past data of a building (at least one year), and \nwill therefore be able to train a model for each building. After the model has \nbeen trained on past data, it can be used to forecast future energy usage, including\nthe effects of any recommendations our team makes to the building owner. \n\n## Linear Modeling\n\nThe best place to start is always with a linear model. These are not complex \nso they cannot capture intricate relationships in the data, but they will give a\ngood first approximation. The first linear model I want to try will use a single \nvariable, temperature. \n\n### Univariate: Energy Consumption versus Temperature\n\nI will segment the data by season and then create a linear model trying to explain daily energy use with temperature as the only explanatory variable.\n\n```{r}\n# Metadata with building location\nmetadata <- read_csv('../metadata/progressive_metadata.csv')\nmetadata <- dplyr::mutate(metadata, location = paste0(City, \", \", State))\nmetadata[which(metadata$Name == 'SRP'), ]$location <- 'Phoenix, AZ 2'\n\n# A simple linear regression model explaining daily energy use by temperature\nlinear_temp_model <- function(df, return_model = FALSE) {\n  # Filter out the summer months and find daily averages\n  df_summer <- filter(df, lubridate::month(timestamp) %in% c(6, 7, 8)) %>%\n    select(timestamp, forecast, temp) %>% mutate(day = lubridate::yday(timestamp)) %>% \n    group_by(day) %>% \n    summarize_all(funs(mean))\n  \n  # Create the model\n  summer_model <- lm(96 * forecast ~ temp, data = df_summer)\n  \n  # Filter out the winter months and find daily averages\n  df_winter <- filter(df, lubridate::month(timestamp) %in% c(12, 1, 2)) %>%\n    select(timestamp, forecast, temp) %>% mutate(day = lubridate::yday(timestamp)) %>% \n    group_by(day) %>% \n    summarize_all(funs(mean))\n  \n  # Create the model\n  winter_model <- lm(96 * forecast ~ temp, data = df_winter)\n  if (return_model) {\n    return(summer_model)\n  }\n  \n  # Return the temp coefficient and the r-squared performance on the training data\n  return(c(summer_model$coefficients[[2]], summary(summer_model)$r.squared,\n           winter_model$coefficients[[2]], summary(winter_model)$r.squared))\n}\n\n# Creata a dataframe to hold all the results\nall_temp_slopes <- as.data.frame(matrix(ncol = 5))\nnames(all_temp_slopes) <- c('bldg', 'summer_slope', 'summer_r2', \n                            'winter_slope','winter_r2')\n\n# Iterate through the buildings and create a summer and winter model for each\nfor (file in filenames) {\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  location <- metadata[which(metadata$Name == name), ]$location\n  results <- linear_temp_model(df)\n  all_temp_slopes <- add_row(all_temp_slopes, bldg = location, \n                             summer_slope = round(results[1], 2), \n                             summer_r2 = round(results[2], 2), \n                             winter_slope = round(results[3],2),\n                             winter_r2 = round(results[4], 2))\n}\n\n# Remove the na row from the dataframe\nall_temp_slopes <- all_temp_slopes[-1,]\n\n\nknitr::kable(all_temp_slopes[1:8, ], caption = 'Summer and Winter Temp Linear Model Stats')\n\n```\n\nThese results have interpretability because I am using a linear model. The slope\nis the change in y for every unit change in x, so the slope represents the change\nin daily energy use (kWh) for a 1 degree Celsius change in temperature. For example,\nfor the SRP building during the summer, with a slope of 100, for a 1 degree increase\nin temperature, this building will use 100 more kWh. At the [national average price of $0.104 /kWh](https://www.eia.gov/electricity/state/),\nthat represents $10.40 in increased cooling costs per day for each degree increase\nin temperature. \n\nTo get a better understanding of this model, we can examine the model results from\none building, the SRP building in Phoenix, Arizona in the American southwest. \n\n```{r, echo = FALSE}\n# Look at the model for a single building\nsrp_df <- read_data(\"f-SRP_weather.csv\")\nsummer_model <- linear_temp_model(srp_df, return_model = TRUE)\n\n# Display the relevant characteristics\nsummary(summer_model)\n```\nThe summary tells us the significance of the variables and the standard error.\nIn this case, the slope is significant with a p value of less than 0.001 and a\nstandard error of 21.52 kWh. The intercept in this case represents the daily\nenergy use for a day with an average temperature of 0 degrees Celsius, not a likely\noccurrence in Phoenix during the summer! \n\nAnother way to visualize the results is to examine the residuals and the \nq-q plot. The base plotting function in R has methods for handling linear models.\n\n```{r, fig.height = 4, fig.width = 6}\n# Plot the results from the model\nplot(summer_model)\n\n# Cook's Distance\nplot(summer_model, which = 4)\n```\n\nThe residuals are relatively evenly distributed around 0\nfor this dataset and do not exhibit a clear systematic bias in either direction. \nThe qqplot also shows that the data is nearly linear although the relationship is not perfect. \nThe Cook's Distance plot shows the points with the highest leverage, or the greatest effect on the model slope. \n\nA better way to visualize the effect of these high-leverage points is to plot the energy\nversus temperature and show a model without\nthe high leverage points and with only the high leverage points. \n\n```{r, echo = FALSE}\n# Create a new dataframe with daily averages during the summer\nsrp_df_summer <- filter(srp_df, lubridate::month(timestamp) %in% c(6, 7, 8)) %>%\n    select(timestamp, forecast, temp) %>% mutate(day = lubridate::yday(timestamp)) %>% \n    group_by(day) %>% \n    summarize(daily_temp = mean(temp, na.rm = TRUE), \n                                daily_energy = 96 * mean(forecast, na.rm = TRUE))\n\n# Highlight the high leverage points\nsrp_df_summer$leverage <- 0\nsrp_df_summer[c(13, 20, 29), 'leverage'] <- 1\n\n# Plot the high leverage points\nggplot(srp_df_summer, aes(daily_temp, daily_energy, color = as.character(leverage))) +\n  geom_jitter(shape = 19, size = 3) + geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"blue\", \"red\"), labels = c('normal', 'high')) + \n  xlab('Temp (degrees C))') + ylab('Daily Energy (kWh)') + labs(color = 'leverage') +\n  ggtitle('Phoenix Energy vs Temp Linear Model') + theme_stata(12) \n```\n\nThis plot shows the effects of the high leverage points on the overall \nrelationship. The high leverage points will tend to decrease the slope \nbetween energy and temperature. \n\nI can print the r_squared values to find how useful temperature is as a predictor\nof daily electricity use in the winter and summer. The [r_squared value (or coefficient of determination) can be interpreted](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/r-squared-or-coefficient-of-determination)\nas the percentage of variation of the dependent variable explained by the \nindependent variable(s) in the model of interest. I already calculated the \nr_squared values for all of the buildings during the summer and winter. \n\nHere are the results.\n\n```{r, echo = FALSE}\n# look at the r-squared values for all models\nall_temp_slopes[c('bldg', 'summer_r2', 'winter_r2')]\n```\n\nIt turns out that temperature alone is not a very good explanation for the daily variance in electricity observed\nin an office building. I will create another linear model that takes into account multiple weather variables\nand compare the results.\n\n### Multivariate: Energy and Multiple Weather Variables\n\nThis model will use all the available weather variables to try and \nexplain the variation in temperature. Again, I will be using the daily \naverages as opposed to every 15-minute interval. \n\n```{r}\n# Fits a linear model to the data using all the weather variables\n# Function returns the r2 value for summer and winter or \n# the entire summer model if specified\nlinear_weather_model <- function(df, return_model = FALSE) {\n  # Select the summer months and the weather columns\n  # Find daily averages\n  df_summer <- filter(df, lubridate::month(timestamp) %in% c(6, 7, 8)) %>%\n    select(timestamp, forecast, ghi, dif, gti, temp, rh, pwat, ws) %>% \n    mutate(day = lubridate::yday(timestamp)) %>% \n    group_by(day) %>% \n    summarize_all(funs(mean))\n  \n  # Create the linear model\n  summer_model <- lm(96 * forecast ~ ghi + dif + gti + temp + rh + pwat + ws, \n                     data = df_summer)\n  \n  # Select the winter months and the weather columns\n  # Find daily averages\n  df_winter <- filter(df, lubridate::month(timestamp) %in% c(12, 1, 2)) %>%\n    select(timestamp, forecast, ghi, dif, gti, temp, rh, pwat, ws) %>%\n    mutate(day = lubridate::yday(timestamp)) %>% \n    group_by(day) %>% \n    summarize_all(funs(mean))\n  \n  # Create the linear model\n  winter_model <- lm(96 * forecast ~ ghi + dif + gti + temp + rh + pwat + ws, \n                     data = df_winter)\n  \n  # Return the summer model if specified\n  if (return_model) {\n    return(summer_model)\n  }\n  \n  # Otherwise return the two r-squared values\n  return(c(summary(summer_model)$r.squared, summary(winter_model)$r.squared))\n}\n\n# Dtaframe to hold reuslts\nweather_r2 <- as.data.frame(matrix(ncol = 3))\nnames(weather_r2) <- c('bldg', 'summer_r2', 'winter_r2')\n\n# Iterate through all buildings and record results\nfor (file in filenames) {\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  results <- linear_weather_model(df)\n  weather_r2 <- add_row(weather_r2, bldg = name,\n                             summer_r2 = round(results[1], 2),\n                             winter_r2 = round(results[2], 2))\n}\n\n# Remove the first row\nweather_r2 <- weather_r2[-1, ]\n\nknitr::kable(weather_r2, caption = 'Weather Linear Model R-squared')\n\n```\n\nIncluding all the weather variables does not improve the model greatly.\nIn order to assess the effect of adding the additional variables, I can run \nan [Analysis of Variance Test (ANOVA)](http://www.statisticshowto.com/anova/)\nto compare the two models. This will tell us if including more variables actually\nimproves the model.\n\n```{r, echo = FALSE}\n# Create the two linear models for the SRP building\nsrp_temp_model <- linear_temp_model(srp_df, return_model = TRUE)\nsrp_weather_model <- linear_weather_model(srp_df, return_model = TRUE)\n\n# Perform an analysis of variance test to determine if the model has improved\nanova(srp_temp_model, srp_weather_model)\n```\n\nThe results show that the second model, with all the weather variables, is slightly\nbetter than the model with only temperature. The residual sum of squares, a measure\nof the total error of the predictions, is lower for the model with all weather\nvariables. However, looking at the p value of 0.2169, this difference is not significant.\nThe interpretation of this p-value is that the observed difference has a 21.69% chance of \noccurring at random. Therefore, we can conclude that including all of the weather\nvariables does not significantly improve the model for predicting average daily\nenergy consumption compared to the baseline with only temperature at a significance level of 0.05. \n\nFinally, we can examine the model with all the weather variables to \ndetermine which have a significant ability to predict energy consumption.\n\n```{r, echo = FALSE}\n# Examine multivariate model\nsummary(srp_weather_model)\n```\n\nThe significant variables in this case are ghi, gti, and temperature. GHI and GTI are both measures\nof irradiance, or the amount of energy from the sun hitting the Earth per unit area. This is potentially\nuseful information as it shows what weather conditions are related to greater \nenergy consumption and could be useful for predicting the weather consumption \nof a building on a given day if the weather forecast is known ahead of time. \nIt is interesting that the gti  coefficient is negative while those for\nghi and temperature are positive as expected. Moreover, temperature is the most\nsignificant variable for this building during the summer. \n\n\nPredicting the average daily energy consumption requires averaging 96 observations. This method \nessentially reduces the amount of data available by a factor of 96 (the number\nof observations per day) because it averages out the variables over each day. \nIn order to get more accurate predictions, I can look at each individual fifteen minute\ninterval of the day and include variables such as the time of day. The resulting predictions\nwill be energy consumption for each fifteen minute interval as opposed to the \ndaily average. The forecast over each interval can then be summed over the course of a day \nto derive the daily energy consumption if needed.\n\n# Model Selection\n\nIn order to develop an optimal model to predict the energy from the weather and time\nvariables for each 15-minute interval, it is best to try out several different approaches.\nAs explained above, the three models I selected were Linear Regression, Support\nVector Machine Regression, and a Random Forest Regressor. The Linear Regression \nwas selected because it is simple and interpretable, the Support Vector Machine \nbecause it has been in common use for this task in the literature, and the Random\nForest because it is a versatile and highly accurate algorithm for non-linear \nproblems. Previous experience with this dataset suggests the relationship between \nenergy use and weather and time is non-linear and will not be explained by a simple\nmodel. \n\nThe only way to choose the best model is to try them out on the data! The basic \nprocedure is to split the data into training and testing sets, fit the models on \nthe training data, make predictions on the test data, and evaluate the models \nuses appropriate metrics. The metrics selected for evaluation are explained below:\n\n1. __rmse__: root mean square error; the square root of the sum of the squared deviations \ndivided by the number of observations. This is a useful metric because it has the same units\n(kWh) as the true value so it can serve as a measure of how many kWh the average\nestimate is from the known true target. \n\n2. __r-squared__: the percentage of the variation in the dependent variable explained \nby the independent variables in the model.\n\n3. __MAPE__: mean average percentage error: the absolute value of the deviation for each \nprediction divided by the true value and multiplied by 100 to convert to a percentage. This metric \nindicates the percentage error \nin the prediction and can be better than the rmse because it takes into account the relative \nmagnitude of the target variable.\n\n\n## Data Preparation\n\nBefore comparing models for energy prediction based, I\nneed to prepare the data for machine learning. This will require several steps:\n\n  * __[One-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) of categorical variables__ \n  * __Transformation of [cyclical variables](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/)__\n  * __[Normalization](http://www.analytictech.com/ba762/handouts/normalization.htm) of variables to zero mean and unit variance __\n  * __Separation into training and testing [features and labels](https://stackoverflow.com/questions/40898019/what-is-the-difference-between-feature-and-label)__\n\nOne hot encoding is a tricky concept at first that is best illustrated with an example.\nThe goal is to convert categorical variables into numeric variables without creating\nan arbitrary ordering. \n\n__One-hot Encoding__\n\nThe process takes this:\n\n| Day of Week |\n|-------------|\n| Monday      |\n| Tuesday     |\n| Wednesday   | \n\n\nand converts it into this:\n\n| Monday | Tuesday | Wednesday |\n|--------|---------|-----------|\n| 1      | 0       | 0         |\n| 0      | 1       | 0         |\n| 0      | 0       | 1         |\n\nEach value for each categorical variable is represented as either a 0 or 1.One-hot encoding is \nrequired because machine learning models do not know how to handle categorical data, and simply\nmapping the values to numbers imposes a random valuing of the feature values based on order, \nwhich may not be appropriate. \n\n__Cyclical variable transformation__ is crucial because some trends, such as months in\na year will not be properly represented by 1-12 numbers. Month 12 is closer to month\n1 than is month 5, but this is falsely depicted by using 1-12. Transforming the variables\ninto cosine and sine components creates the desired relationship between the \nvariables. This is done by using the equation for a sinusoidal \n$$y(t) = sin(2 * \\pi * f * t)$$   In the case of months, the frequency is 1/12 \nbecause the pattern repeats every twelve months (for daily time in hours, the frequency is 1/24). \nThe conversion for months is \ntherefore $$y(m) = sin(2 * \\pi * m / 12)$$ and $$y(m) = cos(2 * \\pi * m/12)$$. \n\nThe result is that month 12 is now represented as occurring closest to month 1 and month 11. \n\n__Normalization__ either means subtracting the \nmean and dividing by the standard deviation or \nsubtracting the minimum value and dividing by the maximum minus\nthe minimum. In the first approach, each feature column will\nhave 0 mean and unit (1) variance. The second approach scales each feature value to between 0 and 1. This step is necessary to remove any\ndisproportionate representations because of the varying units used in variables.\nIn other words, a variable with units of millimeters might have a larger weight attached\nto it than a unit of meters simply because of different units. Normalization prevents \nthis problem of differing unit scales.\n\nFinally, the dataset must be split into __training and testing sets__. During training, the \nmodel is allowed to see the answers in order to learn the relationships (if any)\nbetween the explanatory and response variables. When testing, the model is asked to \nmake predictions for a set of features it has not seen before. The targets for these\nfeatures are known and therefore the performance metrics can be computed based on the \ndiscrepancy between the known target values and the predictions. This is standard \npractice for evaluating a supervised learning algorithm and I will be using\na 0.7/0.3 training/testing split. The training and testing data will be randomly separated. Each model \nwill be evaluated against the same\ntraining and testing data for a fair comparison. \n\nThe following function takes in a dataframe and prepares it completely for\nmodeling. The output will be a dataframe with the variables (features) and a dataframe\nwith the targets (labels). \n\n```{r}\n# Take in a dataframe in standard format and create training and testing sets\nget_features_labels <- function(df) {\n  \n  # Enforce column specifications\n  # Create columns for day of the year, month, and year\n  df2 <- df %>% mutate(day = yday(timestamp), month = month(timestamp), \n                      year = year(timestamp)) %>%  \n    \n    # Transform cyclical features (month, day, num_time)\n    mutate(day_sin = sin(2 * day * pi / 365), \n           day_cos = cos(2 * day * pi / 365),\n           month_sin = sin(2 * month * pi / 12), \n           month_cos = cos(2 * month * pi / 12),\n           num_time_sin = sin(2 * num_time * pi / 24),\n           num_time_cos = cos(2 * num_time * pi / 24))\n  \n  # Convert day of week to a numeric value\n  df2$day_of_week <- as.numeric(unclass(df2$day_of_week))\n  \n  # One hot encoding of categorical variables\n  df2 <- dummy.data.frame(df2, names = c('week_day_end', 'sun_rise_set'), sep = '_') \n  \n  # Extract the known values, and timestamps for graphing\n  labels <- df2[c('timestamp', 'cleaned_energy', 'forecast')]\n  \n  cols_to_remove <- c('elec_cons', 'elec_cons_imp', 'pow_dem', 'cleaned_energy',\n                      'anom_flag', 'forecast', 'anom_missed_flag', \n                      'sun_rise_set_NA')\n  \n  # Change the timestamp to a numeric\n  df2$timestamp <- as.numeric(df2$timestamp)\n  \n  # Remove the columns from the features\n  df2 <- df2[, -which(names(df2) %in% cols_to_remove)]\n  \n  # Scale all the features to have 0 mean and 1 sd\n  df2 <- as.data.frame(scale(df2))\n  \n  # Set a seed for reproducible results\n  set.seed(40)\n  # Split data into training and testing sets with 70% training\n  train_indices <- (createDataPartition(labels$forecast, p = 0.7))$Resample1\n  \n  # Return the complete features and labels as testing and training sets\n  train_features <- df2[train_indices, ]\n  test_features <- df2[-train_indices, ]\n  \n  train_labels <- labels[train_indices, ]\n  test_labels <- labels[-train_indices, ]\n  \n  return(list('X_train' = train_features, 'X_test' = test_features, \n              'y_train' = train_labels, 'y_test' = test_labels))\n}\n\n# Example application\nfeatures_labels <- get_features_labels(df)\nX_train <- features_labels$X_train\nX_test <- features_labels$X_test\ny_train <- features_labels$y_train\ny_test <- features_labels$y_test\n\n# Results in a presentable format\nknitr::kable(X_train[1:5, 1:6], caption = 'Training Features Cols 1:6')\n\nknitr::kable(y_train[1:5, ], caption = \"Training Labels\")\n```\n\n### Switching to Python\n\nA critical aspect of data science is knowing the right tools to use for \nan application. In the case of machine learning implementations, Python handily \nbeats R for ease of use and especially speed. After spending several hours training\na random forest in R, I decided to make the switch to Python. [Scikit-learn](http://scikit-learn.org) \nis an incredible machine learning library built in Python and contains all three models for this report. While it is not yet possible to directly pass R dataframes to Python,\nit can be done using only two additional steps with the [feather](https://github.com/wesm/feather) library. Feather uses binary files to save dataframes and reading and writing times\nare greatly reduced compared to csv files. The entire structure including column names is \npreserved using feather files. \n\n### Feather and Scikit-Learn\n\nThe primary reason to switch to Python is the capabilities and speed of Scikit-learn \nand the switch is made possible by the feather library. To take an R dataframe and \nuse it in Python, the only steps are to write the dataframe to a feather file, and then read\nin the dataframe into a pandas dataframe in Python. Once the models have been built,\nI can export the results to another feather file and read the results back into R\nas a dataframe for analysis. It's not perfect inter-operability, but it's still amazing to \nbe able to use the advantages of each language; the intuitive nature of machine \nlearning implementations in Python, and the charting and data manipulation ability of R. \n\nThe following code creates the training and testing data as R dataframes \nand saves them to feather files. These feather\nfiles can then be read directly into Python as pandas dataframes.\n\n```{r, eval = FALSE}\n# Save training and testing dataframes to feather files for use in Python\nfor (file in filenames) {\n  \n  # Create the training/testing features/labels\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  features_labels <- get_features_labels(df)\n  X_train <- features_labels$X_train\n  X_test <- features_labels$X_test\n  y_train <- features_labels$y_train\n  y_test <- features_labels$y_test\n  \n  # Save the data\n  feather::write_feather(X_train, sprintf('../feather/%s_X_train.feather', name))\n  feather::write_feather(X_test, sprintf('../feather/%s_X_test.feather', name))\n  feather::write_feather(y_train, sprintf('../feather/%s_y_train.feather', name))\n  feather::write_feather(y_test, sprintf('../feather/%s_y_test.feather', name))\n}\n\n```\n\n#### Python Code\n\nFor documentation, here is the Python code used for model evaluation. I like to do\nmy development work in Jupyter Notebooks before moving to Sublime Text for\nforming one coherent script. \n\n```python\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport datetime\nimport time\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVR\nfrom dateutil import parser\nfrom tpot import TPOTRegressor\nimport tensorflow as tf\nimport feather\n\n# Directory with feather files\nroot_dir = 'feather/train_test/'\nbuilding_names = ['APS', 'CoServ', 'Kansas', 'NVE', 'PGE1', 'SDGE', 'SMUD', 'SRP']\n\n# Create the three models with minimal hyperparameter selection\nrf = RandomForestRegressor(n_estimators = 200, max_depth = 40, criterion = 'mse', \nn_jobs = -1, verbose = 1)\nlr = LinearRegression()\nsvr = SVR(kernel = 'rbf', verbose = True)\n\n# Need to store RF feature importances\nfeature_importances = pd.DataFrame(columns = building_names)\n\n# Iterate through all the buildings\nfor building in building_names:\n    \n    # Create a dataframe to hold the predictions\n    predictions = pd.DataFrame(columns = ['rf', 'lr', 'svr'])\n    \n    # Read in the training/testing sets\n    X_train = np.array(feather.read_dataframe(root_dir + \n    '%s_X_train.feather' % building))\n    X_test = np.array(feather.read_dataframe(root_dir + \n    '%s_X_test.feather' % building))\n    y_train = np.array(feather.read_dataframe(root_dir + \n    '%s_y_train.feather' % building)['forecast'])\n    y_train = np.array(feather.read_dataframe(root_dir + \n    '%s_y_train.feather' % building)['forecast'])\n    \n    # Fit (train) the models and make predictions on the test data\n    rf.fit(X_train, y_train)\n    rf_predictions = rf.predict(X_test)\n    \n    # Save features importances of the random forest\n    building_feature_importances = rf.feature_importances_\n    feature_importances[building] = building_feature_importances\n    \n    lr.fit(X_train, y_train)\n    lr_predictions = lr.predict(X_test)\n    \n    svr.fit(X_train, y_train)\n    svr_predictions = svr.predict(X_test)\n    \n    # Save the predictions to a dataframe\n    predictions['rf'] = rf_predictions\n    predictions['lr'] = lr_predictions\n    predictions['svr'] = svr_predictions\n    \n    # Write the predictions \n    feather.write_dataframe(predictions, 'feather/predictions/' + \n    '%s_predictions.feather' % building)\n\n# Write the feature importances to a feather file for analysis\nfeather.write_dataframe(feature_importances, \n'feather/feature_importances/feature_importances.feather')\n\n```\nThe Python code reads the files for each building, trains all three models sing the same training data, and then \nmakes predictions on the test data. The results are stored in pandas dataframes which can then\nbe saved to feather files for plotting metrics in R. A visual model comparison is often more intuitive than merely looking at tables of metrics!\n\n\n```{r, echo = FALSE}\n# Read results back in from Python\nfeather_dir <- '../feather/predictions/'\nmetrics <- as.data.frame(matrix(ncol = 4))\nnames(metrics) <- c('building', 'metric', 'model', 'value')\n\n# Iterate through all the predictions for all the buildings\nfor (file in filenames) {\n  name <- unlist(strsplit(file, '-|_'))[2]\n  \n  # Dataframe to hold results for each building one at a time\n  results <- as.data.frame(matrix(nrow = 9, ncol = 4))\n  names(results) <- c('building', 'metric', 'model', 'value')\n  \n  # Compare the true values and the predictions\n  true_values <- read_feather(sprintf('../feather/train_test/%s_y_test.feather', \n                               name))$forecast\n  predictions <- read_feather(paste0(feather_dir, name, '_predictions.feather'))\n  predictions$true <- true_values\n  \n  # Performance metrics\n  rf_rmse <- Metrics::rmse(predictions$true, predictions$rf)\n  lr_rmse <- Metrics::rmse(predictions$true, predictions$lr)\n  svr_rmse <- Metrics::rmse(predictions$true, predictions$svr)\n  \n  rf_r2 <- cor(predictions$true, predictions$rf) ^ 2\n  lr_r2 <- cor(predictions$true, predictions$lr) ^ 2\n  svr_r2 <- cor(predictions$true, predictions$svr) ^ 2\n  \n  rf_mape <- Metrics::mape(predictions$true, predictions$rf)\n  lr_mape <- Metrics::mape(predictions$true, predictions$lr)\n  svr_mape <- Metrics::mape(predictions$true, predictions$svr)\n  \n  rmse <- c(rf_rmse, lr_rmse, svr_rmse)\n  r2 <- c(rf_r2, lr_r2, svr_r2)\n  mape <- c(rf_mape, lr_mape, svr_mape)\n  all_metrics <- c(rmse, r2, mape)\n  \n  # Add results to building dataframe\n  results$building <- name\n  results$metric <- c(rep('rmse', 3), rep('r2', 3), rep('mape', 3))\n  results$model <- rep(c('rf', 'lr', 'svr'), 3)\n  results$value <- all_metrics\n  \n  # Store all results in a single dataframe\n  metrics <- rbind(metrics, results)\n}\n\n# Remove the first row \nmetrics <- metrics[-1, ]\n```\n\n## Visualization of Performance Metrics\n\nThe performance of each model can viewed as a table, but it is quicker\nand more intuitive to examine charts of the metrics to make comparisons. \nResults presented in these charts were obtained on the test data with the \nalgorithms hyperparameters as shown in the Python code. Each model was trained and tested on the\nsame data to allow for fair comparisons.\n\nThe first plot is the root mean squared error. This metric has the benefit of being in the \nsame units as the target variable and is thus a direct measure if the average\ndeviation of the predictions from the true values.\n\nThe following abbreviations are used in the plots\n\n*lr = linear regression\n*rf = random forest regression\n*svr = support vector machine regressor\n\n```{r, echo = FALSE}\nmetrics <- merge(metrics, metadata[, c('Name', 'location')], by.x = 'building',\n                 by.y = 'Name', all.x = TRUE)\n\n# RMSE Comparison Plot\nggplot(filter(metrics, metric == 'rmse' & building != \"CoServ\" & building != \"SMUD\"), aes(factor(model, levels = c('rf', 'svr', 'lr')))) + \n  geom_bar(aes(y = value, fill = model), \n           color = \"black\", width = 0.9, position = 'dodge', stat = 'identity') + \n  facet_wrap(~location) + xlab('Model') + \n  ylab(\"rmse (kWh)\") + theme_hc(12) + \n  theme(axis.text = element_text(size = 12, color = 'black'),\n        plot.title = element_text(hjust = 0.5)) + \n  scale_y_continuous(breaks = seq(0, 6, 1)) + \n  ggtitle(\"Model Test Set RMSE Comparison\") + \n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\"))\n\n```\n\nThat's one point for the random forest! On each building (I am showing 6 of the 8), \nthe random forest significantly outperforms the other two models. The linear model has\ndouble the inaccuracy of the random forest for nearly every building. \n\nThe next measure is the r squared value. This can be interpreted as the percentage of the \nvariation in the target variable explained by the independent variables in the model. \nFor this situation, it is the variability in the energy consumption for 15-minute interval\nexplained by the corresponding weather and time information.\n\n```{r, echo = FALSE}\n# R Squared Comparison plot\nggplot(filter(metrics, metric == 'r2' & building != \"CoServ\" & building != \"SMUD\"), aes(factor(model, levels = c('rf', 'svr', 'lr')))) + \n  geom_bar(aes(y = value, fill = model), \n           color = \"black\", width = 0.9, position = 'dodge', stat = 'identity') + \n  facet_wrap(~location) + xlab('Model') + \n  ylab(\"r squared\") + theme_hc(12) + \n  theme(axis.text = element_text(size = 12, color = 'black'),\n        plot.title = element_text(hjust = 0.5)) +\n  ggtitle(\"Model Test Set R Squared Comparison\") + \n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\"))\n```\n\nThe second metric is clearly another win for the random forest. On the _test_ set, \nthe random forest achieves over 0.9 R-Squared. All of the models can explain a majority \nof the variance in the energy consumption except for the linear model on the Kansas\nbuilding. The rest of the variation in the response variable is due either to latent\n(hidden) variables, or inherent randomness.\n\nThe last metric is the mean average percentage error. The rmse values are a little \nmeaningless without an idea of the average value for the energy consumption. \nThe mean squared percentage error takes into account the scale of the targets and expresses\nthe error as a percentage of the actual known value.\n\n```{r, echo = FALSE}\n# MAPE Comparison plot\nggplot(filter(metrics, metric == 'mape' & building != \"CoServ\" & building != \"SMUD\"), aes(factor(model, levels = c('rf', 'svr', 'lr')))) + \n  geom_bar(aes(y = 100 * value, fill = model), \n           color = \"black\", width = 0.9, position = 'dodge', stat = 'identity') + \n  facet_wrap(~location) + xlab('Model') + \n  ylab(\"Percentage Error\") + theme_hc(12) + \n  theme(axis.text = element_text(size = 12, color = 'black'),\n        plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Model Test Set MAPE Comparison\") + \n  scale_y_continuous(breaks = seq(0,60, 10)) + \n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\"))\n\n```\n\nAgain, the random forest significantly outperforms the other two models \nachieving less than 10% error on all of the buildings. The linear model is\nmuch less accurate with the support vector machine in the middle in terms of performance. It is \ninteresting that all of the models performed much better on the SRP and APS buildings, which \nare both located in Phoenix. Perhaps the buildings in warmer climates have clearer relationships \nbetween weather and energy usage.\n\nOverall, the random forest demonstrated significantly better performance on \nall three metrics.\n__Therefore, I will select the Random Forest regression model for further development.__\n\n### Runtime\n\nI choose Python because of the reduced run time versus the R implementation of the \nalgorithms. With that in mind, I collected data on the run time of the \nrandom forest in Python because I was interested in the results.\n\n```{r, echo = FALSE}\n\n# Create dataframe of training and prediction times for random forest\nruntimes <- as.data.frame(matrix(nrow = 16, ncol = 3))\nnames(runtimes) <- c('building', 'task', 'time')\nruntimes$building <- c('APS', 'APS', 'CoServ', 'CoServ', 'Kansas', 'Kansas', \n                       'NVE', 'NVE', 'PGE1', 'PGE1', 'SDGE', 'SDGE', 'SMUD', \n                       'SMUD', 'SRP', 'SRP')\nruntimes$task <- rep(c('train', 'predict'), 8)\nruntimes$time <- c(186, 1.7, 156, 2.1, 41.1, 0.4, 108, 1.0, 37.3, 0.3, 47.6, 0.4, \n                   162, 1.5, 102, 0.9)\n\n# Plot the training and prediction times for all builings\nggplot(runtimes, aes(x = building, y = time, \n                     fill = factor(task, levels = c('train', 'predict')))) + \n  geom_bar(stat = 'identity', position = 'dodge') + \n  scale_fill_manual(values = c('blue', 'red')) + xlab('') + ylab('Time (sec)') + \n  ggtitle('Random Forest Runtimes') + labs(fill = 'task') + \n    theme_classic(14) + theme(axis.text = element_text(color = 'black')) + \n  scale_y_continuous(breaks = seq(0, 180, 20))\n```\n\nThe run times are very reasonable even when training\non 100,000 examples. The prediction times are \nespecially impressive as it took less than 5 seconds to make predictions fr each building. \nAll metrics were derived using multiple cores on my personal laptop\nand would likely be much quicker if run in parallel with the computing resources available\nto EDIFES. The runtime of training the models should not be an impediment to production use. \n\n# Cross Validation, Predictive R2\n\nPerformance metrics for the random forest were already computed in the model \nselection part of this report. However, it is worth taking another look \nat the metrics and examining the predictions themselves. \n\n## Performance on Random Train/Test Split\n\nModel selection was done with randomly selected training and testing sets. These\nmetrics are adequate for comparing models, but might need to be adjusted for\nassessing the predictive value of a model. \n\n```{r, echo = FALSE}\n# Find the average metrics for only the random forest\nrf_metrics <- dplyr::filter(metrics, model =='rf') \nrf_metrics_summary <- rf_metrics %>% \n  group_by(metric) %>% summarize(avg = mean(value))\n\n# Display in a table\nknitr::kable(rf_metrics_summary, caption = 'RF Metrics on Random Test Split')\n```\n\nThe final values are very impressive. On the 30% testing random split, the random forest \nregression model achieves an average percentage error of 6.65% and is able to explain 96.8% of the \nvariance in the energy consumption.\n\nWith the predictions and the known labels, we can plot both over time.\nThese points have been chosen at random, so they do not necessarily demonstrate\nany patterns. \n\n```{r, results = 'hide'}\n# Retrieve the training and testing labels\naps_trainlabels <- read_feather('../feather/train_test/APS_y_train.feather')\naps_testlabels <- read_feather('../feather/train_test/APS_y_test.feather')\n\n# Add all the labels into a single dataframe\naps_labels <- rbind(aps_trainlabels, aps_testlabels) %>% arrange(timestamp)\naps_predictions <- read_feather('../feather/predictions/APS_predictions.feather')\n\n# Add the timestamp back to the predictions\naps_predictions$timestamp <- aps_testlabels$timestamp\n\n# Get the predictions and true values in the same dataframe\naps_comparison <- merge(aps_labels, aps_predictions, by = c('timestamp'), all.x = TRUE)\n\n# Plot the true values and the predictions\np <- ggplot(aps_comparison, aes(as.Date(timestamp), forecast)) + geom_point(alpha = 0.1)  + \n  geom_point(aes(y = rf), color = 'red', size = 1.1, alpha = 0.1) + xlab('') + ylab('Energy (kWh)') +\n  ggtitle('RF APS Random Split Predictions') + theme_classic(14)\n\n```\n\n```{r, echo = FALSE}\nknitr::include_graphics('APS_random_pred.png')\n```\n\nMaybe a better plot is to plot the residuals over time to see if they\nexhibit any noticeable trends. This could allow me to see if the model is \nconsistently under or over predicting for example.\n\n```{r, echo = FALSE}\naps_residuals <- aps_comparison %>% \nselect(timestamp, forecast, rf) %>% \n  filter(!(is.na(rf))) %>% mutate(residual = forecast - rf)\n\n\nggplot(aps_residuals, aes(timestamp, residual)) + \n  geom_point(col = 'blue', alpha = 0.5) + geom_smooth(se = FALSE, col = 'red') + \n  ylab('Residual') + ggtitle('RF APS Residuals')\n```\n\nThe residuals do not seem to indicate any particular skew. They are evenly distributed\naround 0 showing that the model over and under predicts at about the same frequency.\n\n## Hyperparameter Optimization\n\nNow that the random forest has clearly been labeled the best algorithm for this\ntask, I need to work on optimizing the model. The random forest has a number\nof hyperparameters (think of settings) that can be adjusted to develop the best model for\na given set of training data. Hyperparameters differ from parameters in that\nthe latter are learned by the model during training while hyperparameters\nmust be explicitly defined by the model creator before training. Scikit-learn implements\na set of sensible default hyperparameters, but there is no guarantee these will be the \nbest settings for a particular problem. \n\nHyperparameters are best optimized by creating a number of different models and \nevaluating each one on the same testing set, or using cross-validation to select\nthe optimal model. Scikit-learn has a number of handy tools for hyperparamter\noptimization including a grid search which exhaustively tries every model \nin a specified grid and evaluates each one using cross-validation. The function \nreturns the performance of all models and the best performing model can be selected for \napplication. The following Python code performs this grid search over a pre-defined \ngrid of hyperparameters. \n\n#### Python Code\n\n```python\n# Perform grid search to find best parameters\n# Feature selection, depth, number of trees, and minimum size of leaves\nparam_grid = {\n    'max_features': [None, 0.5], \n    'max_depth': [None, 40],\n    'n_estimators':[200, 400],\n    'min_samples_leaf': [1, 4]\n}\n\n# Testing on a dataset with a high mape to improve performance\nnve_X_train = feather.read_dataframe('feather/train_test/NVE_X_train.feather')\nnve_X_train = np.array(nve_X_train)\n\nnve_X_test = feather.read_dataframe('feather/train_test/NVE_X_test.feather')\nnve_X_test = np.array(nve_X_test)\n\nnve_y_train = feather.read_dataframe('feather/train_test/NVE_y_train.feather')\nnve_y_train = np.array(nve_y_train['forecast'])\n\nnve_y_test = feather.read_dataframe('feather/train_test/NVE_y_test.feather')\nnve_y_test = np.array(nve_y_test['forecast'])\n\nrf_test = RandomForestRegressor(bootstrap = True, verbose = 2, n_jobs = -1)\n\nrf_grid = GridSearchCV(rf_test, param_grid, scoring = 'neg_mean_squared_error', n_jobs = -1, \nverbose = 2)\n\nrf_grid.fit(nve_X_train, nve_y_train)\n\nrf_grid.best_params_\n```\n\nThe best parameters are as follows: \n\n* 200 decision trees in the forest\n* No maximum depth for each tree (trees will be grown until all predictions are made)\n* A minimum of 1 sample per leaf node\n* A maximum fraction of 50% of the features evaluated for each tree\n* A minimum number of 2 samples to split a node\n* A minimum impurity decrease of 0.0 to split a node\n\nOnce the optimal hyperparameters have been selected, I can evaluate them against the \nsame training and testing set as the baseline set of hyperparameters to assess the \ndifference in performance.\nThe optimized hyperparameters were selected based on a single building (NVE building) \nand therefore I need to make sure that these settings will work well for all buildings.\nThe following procedure to assess performance is basically the same as for the base models:\nread in the predictions and true values, calculate the performance metrics, and \ngraph the results for a \na comparison.If the optimized model performs better on average with the same training \nand testing set, then I can conclude that these hyperparameters are a better fit for the problem. \n\n\n### Optimized to Baseline Comparison\n\n```{r, echo = FALSE}\n# Read results back in from Python\nfeather_dir <- '../feather/predictions/'\nopt_metrics <- as.data.frame(matrix(ncol = 3))\nnames(opt_metrics) <- c('building', 'metric','value')\n\n# Iterate through all the predictions for all the buildings\nfor (file in filenames) {\n  name <- unlist(strsplit(file, '-|_'))[2]\n  \n  # Dataframe to hold results for each building one at a time\n  results <- as.data.frame(matrix(nrow = 3, ncol = 3))\n  names(results) <- c('building', 'metric', 'value')\n  \n  # Compare the true values and the predictions\n  true_values <- read_feather(sprintf('../feather/train_test/%s_y_test.feather', \n                               name))$forecast\n  predictions <- read_feather(paste0(feather_dir, name, '_optimized_predictions.feather'))\n  \n  predictions$true <- true_values\n  \n  # Performance metrics\n  rf_rmse <- Metrics::rmse(predictions$true, predictions$rf)\n  \n  rf_r2 <- cor(predictions$true, predictions$rf) ^ 2\n  \n  rf_mape <- Metrics::mape(predictions$true, predictions$rf)\n  \n  all_metrics <- c(rf_rmse, rf_r2, rf_mape)\n  \n  # Add results to building dataframe\n  results$building <- name\n  results$metric <- c('rmse', 'r2', 'mape')\n  results$value <- all_metrics\n  \n  # Store all results in a single dataframe\n  opt_metrics <- rbind(opt_metrics, results)\n}\n\n# Remove the first row \nopt_metrics <- opt_metrics[-1, ]\nopt_metrics <- merge(opt_metrics, metrics[, c('building', 'location')], \n                     by = 'building', all.x = TRUE)\n```\n\n\nIn order to determine if the hyperparameter selection actually improved performance,\nI need to compare these results to those previously obtained with the baseline random\nforest. That can be done quantitatively with tables and visually with plots.\n\n```{r, echo = FALSE}\n# Create a dataframe of the optimized rf results\nrf_metrics <- rf_metrics\nrf_metrics$model <- 'base'\nopt_metrics$model <- 'opt'\nrf_metrics <- rbind(rf_metrics, opt_metrics)\n\n# Plot the rmse of optimized versus base model\nggplot(dplyr::filter(rf_metrics, metric == 'rmse'), aes(location, value, \n                                                       fill = model)) + \n         geom_bar(stat = 'identity', position = 'dodge', color = 'black') + \n         scale_fill_stata() + \n         xlab('Building') + ylab('RMSE (kWh)') + \nggtitle('Baseline vs Optimized RF RMSE Comparison') + theme_hc(12) + \n  geom_text(data = dplyr::filter(rf_metrics, metric == 'rmse' & model == 'opt'),\n            aes(label = round(value, 2)), vjust = -0.4, hjust = -.2, size = 3) + \n  geom_text(data = dplyr::filter(rf_metrics, metric == 'rmse' & model == 'base'),\n            aes(label = round(value, 2)), vjust = -0.4, hjust = 1, size = 3) + \n  theme(plot.title = element_text(hjust = 0.5), \n        axis.text.y = element_text(color = 'black'), \n        axis.text.x = element_text(color = 'black', angle = 45, vjust = 0.5))\n```\n\nThe optimized random forest outperforms the base model on root-mean-squared-error\nfor all of the buildings using a random test set. This indicates that the hyperparameter tuning has\nimproved the model's predictive capability. We can also plot the mean percentage error \nfor comparison.\n\n```{r}\n# Plot the MAPE for the base and optimized model\nggplot(dplyr::filter(rf_metrics, metric == 'mape'), aes(building, value * 100, \n                                                       fill = model)) + \n         geom_bar(stat = 'identity', position = 'dodge', color = 'black') + \n         scale_fill_manual(values = c('darkgreen', 'blue')) + \n         xlab('Building') + ylab('MAPE (%)') + \nggtitle('Baseline vs Optimized RF MAPE Comparison') + theme_dark(12) + \n  theme(panel.grid = element_blank(), axis.text = element_text(face = 'bold', color = 'black')) + \n  geom_text(data = dplyr::filter(rf_metrics, metric == 'mape' & model == 'opt'),\n            aes(label = round(value, 4) * 100), size = 3, vjust = -0.4, hjust = -.2) + \n  geom_text(data = dplyr::filter(rf_metrics, metric == 'mape' & model == 'base'),\n            aes(label = round(value, 4) * 100), size = 3, vjust = -0.4, hjust = 1)\n```\nBased on the root mean squared error and the mean average percentage error, \nI can conclude that the optimized model does outperform the baseline model using a \nrandom training/testing split.\nMoreover, the optimized model does not take noticeably longer to run than the \nbaseline model. Therefore, there is no reason to use the baseline model, and I will adopt\nthe optimized model for all future implementations of the random forest regressor. \n\n# Interpret Results\n\nWe can learn useful information from both the linear modeling and the random forest \nregression. These models can tell us what variables are most important in \ndetermining electricity usage and how increasing one variable\nmay affect the energy consumption of a building. \n\n## Linear Model Interpretation \n\nFirst, we can look at the results of the univariate linear regression \nto see the effects of temperature on daily energy usage. Using the SMUD building,\nthe coefficient between daily average energy consumption and daily average temperature \nwas 308. For this building, an increase of 1 degree Celsius in temperature during the summer\nresults in 308.40 kWh more energy consumption. This represents about $40 of electricity\nand is an intriguing result demonstrating the effect of higher temperatures. \n\nMoreover, from the multivariate modeling using all the weather variables and \ndaily average energy, we were able to determine the three significant variables\nas temperature, ghi, and gti. All other weather\nvariables were not useful for predicting the average daily energy consumption.\nOverall, the linear models developed to predict daily average energy consumption from weather\nvariables were not very explanative, with an average r-squared around 0.20. \nThis indicates that only about 20% of the variability in daily\nenergy consumption can be explained by variability in weather consumption.\n\n## Random Forest Interpretation\n\nThe random forest is able to return feature importances that show the relative importance\nof each feature. These are not absolute measures (they represent the reduction in \nerror attributed to the variable) but the numbers are useful for comparison. \n\nFirst, we can look at the feature importances for all buildings at the same time. \n\n```{r}\n\n# Feature Importances\nfeatures_import <- read_feather('../feather/feature_importances/feature_importances.feather')\n\nfeatures <- names(read_feather('../feather/train_test/APS_X_test.feather'))\n\nfeatures_import$feature <- features\n\nfeatures_import <- reshape2::melt(features_import, id.vars = c('feature'))\nnames(features_import) <- c('feature', 'building', 'importance')\n\nfeatures_import <- merge(features_import, metadata[, c('Name', 'location')], \n                         by.x = 'building', by.y = 'Name', all.x = TRUE)\n\n# Plot the feature importances for all buildings\nggplot(features_import, aes(feature, importance, col = location)) + \n  geom_point(size = 3) + xlab('feature') + ylab('RF Importance') + \n  ggtitle('Random Forest Feature Importances') +  \n  theme(axis.text.x = element_text(angle = 90, vjust = 0)) + \n  theme_hc(12) + scale_color_tableau() + \n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'right')\n\n\n\n```\n\nMuch as the linear model demonstrated, the most helpful weather variables are temperature and \nghi. In terms of all the variables, the time of day in hours (num time) as well as the \ncyclical transformation of this feature proved to be significant in predicting energy \nconsumption. The weekday indicator \nalso was significant, which is not unexpected because office buildings show a significant\ndifference in energy consumption on the weekends as compared to during the week. \nFinally, we can also see that many features might not be useful and could be \nexcluded from future models such as month, and sun rise or sun set.\n\nI can then take the average of the feature importances across all buildings for a \nbetter comparison.\n\n```{r}\n# Show the average importance of features across all buildings\naverage_import <- features_import %>% group_by(feature) %>%\n  summarize(average = mean(importance)) %>%\n  arrange(desc(average))\n\n# Plot as a horizontal barchart\nggplot(average_import, aes(reorder(feature, average), average)) + \n  geom_bar(stat = 'identity', fill = 'firebrick', withd = 0.9, \n           color = 'black', lwd = 1) + \n  coord_flip() + xlab('') + ylab('Importance') + \n  ggtitle('Average RF Feature Importance') + theme_hc(12) + \n  scale_y_continuous(lim = c(0, 0.3), breaks = seq(0, 0.3, 0.05)) + \n  theme(axis.text = element_text(color = 'black')) +\n  geom_text(aes(y = average + 0.015, label = round(average, 3)))\n```\n\nThis chart is pretty definitive! The temperature is clearly the most significant\nvariable, followed by whether or not the day is a weekday or weekend, the time of day in hours,\nand whether or not the day is a business day (which accounts for holidays as well as \nweekends). Overall, the most significant weather variables are temperature, dif,\ngti, and ghi. There are a number of unimportant variables, such as windspeed, month \n(including all cyclical transformations), year, and sun rise or set. This last one\nis not surprising because the vast majority of times are not sunrise or sunset,\nand these have no noticeable connection with energy usage. These are low variance variables \nthat do not have much explanatory power. They should most likely be removed\nfrom any model to avoid adding irrelevant features. \n\nRemoving unnecessary features can improve the performance of a model as well as \ndecreasing the run time. The optimized random forest only considered 50% of the features \nfor each tree and achieved slightly better results than the base random forest.\nWhether or not this is due to the features retained could be determined through testing,\nbut it is clear that not all features need to be retained in order to predict energy use. Moreover,\nthe best model could have faster run times if some of the features were entirely removed.\n\n### Visualizing A Single Decision Tree\n\nAnother incredible aspect of the Scikit-learn implementation of the random forest\nis that it is possible to examine every single tree in the forest. This makes it  possible\nto determine exactly why a particular tree made the classification/prediction that it did. \nThe complete trees in the actual random forest can be hundreds of layers deep, so I will\ncreate a simplified tree with a depth of three to illustrate how a decision tree \nis able to make an energy prediction. Theoretically, we could reconstruct the actual values\nof the features (before they have been transformed) to interpret the exact\nset of circumstances (weather/time) that lead to a predicted amount of energy consumption.\nThis process would be arduous but could give us extreme detailed insight into the factors\nthat cause a building to use more energy. A later part of this project could involve \nexamining the trees in order to extract usable data from the prediction reasoning.\n\n```{r, echo = FALSE}\nknitr::include_graphics('../treevis_verysmall.png')\n```\n\nBecause the features have been scaled, the tree loses some interpretability, but it would \nbe possible to reverse the preprocessing steps to extract the true values of the features. \nThe tree has also been simplified so it  can be displayed as the actual trees require \nmany additional layers to make a prediction based on the provided features. \n\n## Simulating Global Warming\n\nThe final part of interpreting results I would like to try is to see what happens\nwith the change of a single variable. In this case, I will increase the average temperature\nby 2 degrees Celsius to simulate the amount of global warming expected to occur\nby the end of the century. We can then see what effect this will have on the energy \nconsumption of the buildings. This is a simple transformation and I am eager to see the \nresults. Perhaps it will turn out that increasing the temperature increases building energy \nuse in some locations (particularly the southwest) but decreases it in other areas \nthat have to use less electricity during the winter for heating. \n\n```{r, results = 'hide'}\n# Take in a dataframe in standard format and create training and testing sets\nget_features_labels_notest <- function(df) {\n  \n  # Enforce column specifications\n  # Create columns for day of the year, month, and year\n  df2 <- df %>% mutate(day = yday(timestamp), month = month(timestamp), \n                      year = year(timestamp)) %>%  \n    \n    # Transform cyclical features (month, day, num_time)\n    mutate(day_sin = sin(2 * day * pi / 365), \n           day_cos = cos(2 * day * pi / 365),\n           month_sin = sin(2 * month * pi / 12), \n           month_cos = cos(2 * month * pi / 12),\n           num_time_sin = sin(2 * num_time * pi / 24),\n           num_time_cos = cos(2 * num_time * pi / 24))\n  \n  # Convert day of week to a numeric value\n  df2$day_of_week <- as.numeric(unclass(df2$day_of_week))\n  \n  # One hot encoding of categorical variables\n  df2 <- dummy.data.frame(df2, names = c('week_day_end', 'sun_rise_set'), sep = '_') \n  \n  # Extract the known values, and timestamps for graphing\n  labels <- df2[c('timestamp', 'cleaned_energy', 'forecast')]\n  \n  cols_to_remove <- c('elec_cons', 'elec_cons_imp', 'pow_dem', 'cleaned_energy',\n                      'anom_flag', 'forecast', 'anom_missed_flag', \n                      'sun_rise_set_NA')\n  \n  # Change the timestamp to a numeric\n  df2$timestamp <- as.numeric(df2$timestamp)\n  \n  # Remove the columns from the features\n  df2 <- df2[, -which(names(df2) %in% cols_to_remove)]\n  \n  # Scaling will be done in Python\n  return(list('features' = df2,\n              'labels' = labels))\n}\n\nincrease_temp <- function(df) {\n  \n  # Increase the temp by 2 degrees Celsius to simluate global warming\n  df2 <- df %>% mutate(temp = temp + 2)\n  \n  # Pass through function to get features and labels\n  train <- get_features_labels_notest(df)\n  test  <- get_features_labels_notest(df2)\n  \n  return(list('train' = train$features, \n              'test'  = test$features, \n              'labels' = train$labels))\n}\n\n\n# Example application\nsrf_df <- read_data('f-SRP_weather.csv')\nincreased_df <- increase_temp(srp_df)\n```\n\n```{r eval = FALSE}\n# Create training and testing feather files for Python\nfor (file in filenames) {\n  \n  # Create the training/testing features/labels\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  increased_results <- increase_temp(df)\n  \n  train <- increased_results$train\n  labels <- increased_results$labels\n  test  <- increased_results$test\n  \n  # Train is original dataframe modified for machine learning, labels are forecasted \n  # Test is same dataframe but increased temperature by 2 degree\n  feather::write_feather(train, sprintf('../feather/increased/%s_train.feather', name))\n  feather::write_feather(labels, sprintf('../feather/increased/%s_labels.feather', name))\n  feather::write_feather(test, sprintf('../feather/increased/%s_test.feather', name))\n}\n```\n\nAfter creating the dataframes, we send them to Python for prediction using the \nrandom forest with the optimized parameters. The random forest is trained on\nthe normal data, and then makes predictions on the increased temperature\ndata. We can then read these prediction back in to assess the effects of \nthis modest global warming estimate. \n\n```{r, echo = FALSE}\n# Create a vector to hold results\ndiff_df <- as.data.frame(matrix(ncol = 4))\nnames(diff_df) <- c('building', 'before', 'after', 'diff_pct')\n\nyearpoints <- 96 * 365\n\n# Iterate through the file and find the differences\nfor (file in filenames) {\n  name <- unlist(strsplit(file, '-|_'))[2]\n  \n  # Find the total consumption before, total consumption after, and change\n  predictions <- read_feather(sprintf('../feather/predictions/%s_increased_predictions.feather', name))\n  true <- read_feather(sprintf('../feather/increased/%s_labels.feather', name))\n  \n  if (length(predictions) >= yearpoints) {\n    last_index <- length(predictions) - yearpoints\n    annual_before <- sum(true$forecast[last_index:nrow(true)])\n    annual_after <- sum(predictions[last_index:length(predictions)])\n    \n  } else {\n    annual_before <- sum(true$forecast)\n    annual_after <- sum(predictions)\n  }\n  \n  difference_pt <- (annual_after - annual_before) / annual_before * 100\n  \n  # Add the row to a dataframe for record keeping\n  diff_df <- add_row(diff_df, building = name, before = annual_before,\n                     after = annual_after, diff_pct = difference_pt)\n\n}\n\n# Only complete rows retained\ndiff_df <- diff_df[complete.cases(diff_df), ]\n\n# Record increase for coloring\ndiff_df$increase <- ifelse(diff_df$diff_pct > 0, 'red', 'green')\ndiff_df <- merge(diff_df, metadata[, c('Name', 'location')], \n                 by.x = 'building', by.y = 'Name', all.x = TRUE)\n\n# Plot the results\nggplot(diff_df, aes(x = location, y = diff_pct, fill = increase)) + \n  geom_bar(stat = 'identity', color = 'black') + \n  xlab('') + ylab('% Difference') + \n  ggtitle('Effect of 2 C Increase in Temp') + \n  scale_y_continuous(lim = c(-2, 5), breaks = seq(-2, 5, 1)) + \n  theme_economist(12) + \n  scale_fill_identity() + theme(legend.position = 'none')\n```\nWhile some buildings will see less energy use, 6 out of the 8 will see increased\nelectricity consumption due to a 2 degree C increase in temperature. \nLet's quantify those results in terms of actual dollars. The average price per \nkWh of electricity in the United States is $0.104 (which will only go up \nin the future unless renewables completely take over the grid). \n\n```{r}\n# Calculate and graph costs (or savings) of temp increase\nkwh_cost <- 0.104\ndiff_df <- diff_df %>% mutate(cost = (after - before) * kwh_cost)\n\n# PLot the costs (using economist theme of course)\nggplot(diff_df, aes(location, cost, fill = increase)) + \n  geom_bar(stat = 'identity') + \n  scale_fill_identity() + xlab('') + ylab('Cost $') + \n  ggtitle('Annual Cost of 2 C Increase in Temp') + theme_economist(12) +\n  scale_y_continuous(breaks = seq(0, 30000, 5000), limits = c(-500, 35000)) + \n  geom_text(label = round(diff_df$cost), vjust = -1) + \n  theme(axis.text.x = element_text(angle = 60, vjust = 0.5),\n        plot.title = element_text(hjust = 0.5))\n```\n\nI hope the owner of the SMUD building believes in global warming because \nshe/he will be in for quite a surprise if something is not done. Overall, we can \nsay that among this sample of eight Progressive building, the total cost in \nenergy use from the 2 degree Celsius increase in temperature is __$40191.25__\nevery year. There may be some people who dismiss science, but economics is an area in which\nwe are all in agreement; less cost is better.\n\nThe last graph in this section is of the daily average energy consumption before and \nafter the 2 C increase for the SMUD building in Sacramento, CA. For nearly every day \nof the year, the average energy consumption is greater with the higher temperature. \n\n```{r, echo = FALSE}\n# Read in the actual energy values and the predictions \n# from the increased temperature model\nsmud_df <- read_data('f-SMUD_weather.csv')\nsmud_predictions <- read_feather('../feather/predictions/SMUD_increased_predictions.feather')\nsmud_df$in_pred <- smud_predictions$predictions\nsmud_df$day <- lubridate::yday(smud_df$timestamp)\n\n# Create a long dataframe with daily averages for plotting\nsmud_increase <- smud_df %>% select(day, forecast, in_pred) %>% group_by(day) %>%\n  summarize_all(funs(mean)) %>% gather(key = 'type', value = 'energy', -day)\n\n# Plot the average daily consumption with and without the temperature change\nggplot(smud_increase, aes(x = day, y = energy, col = type)) + \n  geom_line(lwd = 1.1) + xlab('') + ylab('Energy Consumption') + \n  theme_solarized(12) + ggtitle('Daily Average Energy Comparison with 2 C Increase') + \n  scale_color_manual(values = c('darkgreen', 'orange'), \n                                              labels = c('Before', 'After')) + \n  theme(title = element_text(color = 'black'), axis.text = element_text(color = 'black'),\n        axis.title = element_text(color = 'black'), legend.text = element_text(color = 'black'),\n        legend.title = element_blank())\n```\n\nThese results demonstrate the value of the random forest model. We can replay the \ndays in the dataset and alter as many variables as desired to assess the effects of\nchanges. The next step for this approach would be to find a way to model building retrofits\nto determine energy savings of particular recommendations. \n\n# Challenge Results\n\nTo challenge the random forest regression model, I wanted to see if it could perform accurate \nprediction. One of the primary objectives of EDIFES is to develop a model that can\nforecast future energy use both before and after building modifications. In order to \nverify the accuracy of a forecast model, we must be able to construct a model \nwhich achieves an adjusted r-squared of greater than 0.85 when predicting six months\nof energy use. This model will train on all available data, and then make predictions\non the final six months of the data which can then be compared to the known values. \n\n\n## Prediction of Six Months \n\nIn order to test the model, I will try to predict the last six months of energy \nconsumption from the weather and time variables for that time period. \nThe random forest requires the same number of features during testing as during training, and\nso will always need the full provided set of features. When we need to make forecasts\nwithout known weather data, we will have to use average weather from the past few years\nor the typical meteorological year, which is based on decades of weather and \nhence is not very accurate because of the recent changes to weather from\nglobal warming. The six month prediction is meant to validate the accuracy\nof the model when the weather is known.\n\nThe predictions on the testing data can\nthen be compared to the actual answers (the true energy consumption) in order to \ndetermine the prediction ability of the random forest. Because EDIFES will always be able\nto retrieve historical weather data for a location, this approach could be implemented to \nforecast into the future if it proves successful. We would need to train a model using the \nexisting weather and electricity consumption, match the weather data from the previous year\n(or an average of multiple previous years)\nto the future dates we want to forecast, and then have the model make predictions for those\ndates. Eventually, we would have to develop a method for simulating improving insulation,\naltering an HVAC schedule, or any number of steps taken to decrease energy use.\n\nThe first step is to create training/testing sets with the final 6 months used for\ntesting. The code below removes creates training and testing features and labels for\nthe final six months of each dataset.\n\n```{r}\n# Take in a dataframe in standard format and create training and testing sets \n# of final six months\nprepare_six_months <- function(df) {\n  \n  # Enforce dataframe\n  df <- as.data.frame(df)\n  \n  # Enforce column specifications\n  # Create columns for day of the year, month, and year\n  df2 <- df %>% mutate(day = yday(timestamp), month = month(timestamp), \n                       year = year(timestamp)) %>%  \n    \n    # Transform cyclical features (month, day, num_time)\n    mutate(day_sin = sin(2 * day * pi / 365), \n           day_cos = cos(2 * day * pi / 365),\n           month_sin = sin(2 * month * pi / 12), \n           month_cos = cos(2 * month * pi / 12),\n           num_time_sin = sin(2 * num_time * pi / 24),\n           num_time_cos = cos(2 * num_time * pi / 24))\n  \n\n  # Enforce factors\n  df2$day_of_week <-  as.factor(df2$day_of_week)\n  df2$week_day_end <- as.factor(df2$week_day_end)\n  df2$sun_rise_set <- as.factor(df2$sun_rise_set)\n  \n  # One hot encoding of categorical variables\n  df2 <- dummy.data.frame(df2, names = c('day_of_week', \n                                         'week_day_end', 'sun_rise_set'), sep = '_') \n  \n  # Extract the known values, and timestamps for graphing\n  labels <- df2[c('timestamp', 'cleaned_energy', 'forecast')]\n  \n  cols_to_remove <- c('elec_cons', 'elec_cons_imp', 'pow_dem', 'cleaned_energy',\n                      'anom_flag', 'forecast', 'anom_missed_flag')\n  \n  # Change the timestamp to a numeric\n  df2$timestamp <- as.numeric(df2$timestamp)\n  \n  # Remove the columns from the features\n  df2 <- df2[, -which(names(df2) %in% cols_to_remove)]\n  \n  # Index of dates for final six months\n  six_months_index <- nrow(df2) - (60 / as.numeric(df$timestamp[5] - \n                                                     df$timestamp[4])) * 24 * 180\n  \n  # Return the complete features and labels as testing and training sets\n  train_features <- df2[1:six_months_index - 1, ]\n  test_features <- df2[six_months_index:nrow(df2), ]\n  \n  # Scale features\n  # Train the prepreocessor on training data and then transform\n  # both training and testing data\n  pre_processor <- caret::preProcess(train_features, \n                                     method = c('center', 'scale', 'nzv'))\n  \n  train_features <- predict(pre_processor, train_features)\n  test_features <- predict(pre_processor, test_features)\n  \n  # Extract labels\n  train_labels <- labels[1:six_months_index - 1, ]\n  test_labels <- labels[six_months_index:nrow(df2), ]\n  \n  return(list('X_train' = train_features, 'X_test' = test_features, \n              'y_train' = train_labels, 'y_test' = test_labels))\n}\n\n# Example Application\ndf <- read_data('f-SMUD_weather.csv')\nname <- 'SMUD'\n\nfeatures_labels <- prepare_six_months(df)\nX_train <- features_labels$X_train\nX_test <- features_labels$X_test\ny_train <- features_labels$y_train\ny_test <- features_labels$y_test\n\n```\n\nNow, I will use the function to create the training and testing features and labels. \nAgain, these will be saved as feather files so they can be read into Python \nand the random forest trained on the data. \n\n```{r eval = FALSE, include = FALSE}\n# Save training and testing dataframes to feathers for use in Python\nfor (file in filenames) {\n  # Create the training/testing features/labels\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  features_labels <- prepare_six_months(df)\n  X_train <- features_labels$X_train\n  X_test <- features_labels$X_test\n  y_train <- features_labels$y_train\n  y_test <- features_labels$y_test\n  \n  # Check to make sure all sets look correct\n  print(nrow(y_test))\n  print(nrow(X_test))\n  print(sum(is.na(X_test)))\n  print(sum(is.na(X_train)))\n  print(sum(is.na(y_train)))\n  print(sum(is.na(y_test)))\n\n  # Save the data\n  feather::write_feather(X_train, sprintf('../feather/challenge_predictions/%s_X_train.feather', name))\n  feather::write_feather(X_test, sprintf('../feather/challenge_predictions/%s_X_test.feather', name))\n  feather::write_feather(y_train, sprintf('../feather/challenge_predictions/%s_y_train.feather', name))\n  feather::write_feather(y_test, sprintf('../feather/challenge_predictions/%s_y_test.feather', name))\n}\n```\n\nThe Python code to read in the data, train the model, and make predictions using the \nmodel is much the same as before (in fact it is exactly the same except for the names\nof the input files!). The predictions are recorded in pandas dataframes and written to \nfeather files for transport back to R and evaluation. \n\n### Assessing Prediction Performance\n\nBy comparing the predictions for the final six months with the known values of energy use,\nI can evaluate the performance of the model. I will again use the 3 metrics outlined\nprior (rmse, r2, and mape) to determine if this model can make accurate predictions.\n\n```{r, echo = FALSE}\n# Read results back in from Python\nfeather_dir <- '../feather/challenge_results/'\nmonths_metrics <- as.data.frame(matrix(ncol = 3))\nnames(months_metrics) <- c('building', 'metric', 'value')\n\n# Iterate through all the predictions for all the buildings\nfor (file in filenames) {\n  name <- unlist(strsplit(file, '-|_'))[2]\n  \n  # Dataframe to hold results for each building one at a time\n  results <- as.data.frame(matrix(nrow = 3, ncol = 3))\n  names(results) <- c('building', 'metric', 'value')\n  \n  # Compare the true values and the predictions\n  true_values <- read_feather(sprintf('../feather/challenge_predictions/%s_y_test.feather', \n                               name))$cleaned_energy\n  \n  # Read in the predictions and add the true values to the dataframe\n  predictions <- read_feather(paste0(feather_dir, name, '_preds.feather'))\n  predictions$true <- true_values\n  \n\n  rf_rmse <- Metrics::rmse(predictions$true, predictions$predictions)\n  rf_r2 <- cor(predictions$true, predictions$predictions) ^ 2\n  rf_mape <- Metrics::mape(predictions$true, predictions$predictions)\n  \n  all_metrics <- c(rf_rmse, rf_r2, rf_mape)\n  \n  # Add results to building dataframe\n  results$building <- name\n  results$metric <- c('rmse', 'r2', 'mape')\n  results$value <- all_metrics\n  \n  # Store all results in a single dataframe\n  months_metrics <- rbind(months_metrics, results)\n}\n\n# Dataframe of results for the final 4 months\nmonths_metrics <- months_metrics[complete.cases(months_metrics), ]\nmonths_metrics <- merge(months_metrics, metadata[, c('Name', 'location')],\n                        by.x = 'building', by.y = 'Name', all.x = TRUE)\n```\n\nThe moment of truth is visualizing these metrics. \n\n```{r, echo = FALSE}\n# Convert MAPE to a percentage\nmonths_metrics[which(months_metrics$metric == 'mape'), 'value'] <- \n  months_metrics[which(months_metrics$metric == 'mape'), 'value'] * 100\n\nknitr::kable(months_metrics, caption = 'RF Prediction Metrics')\n\n# Metric Plots for the final Six Months\nggplot(filter(months_metrics, building != \"CoServ\" & building != \"SMUD\"), \n       aes(x = location, y = value)) + \n  geom_bar(stat = 'identity', fill = 'gray20', color = 'black') +\n  ylab(\"\") + xlab('Building') + \n  facet_grid(metric ~ ., scales = 'free_y') + \n  ggtitle(\"Random Forest Prediction Performance\") + theme_hc(12) + \n  theme(axis.text = element_text(color = 'black'), \n        plot.title = element_text(hjust = 0.5))\n\nmonths_metrics[which(months_metrics$value == Inf), ]$value <- NA\nprediction_summary <- group_by(months_metrics, metric) %>%\n  summarize(value = mean(value, na.rm = TRUE))\n```\n\nThese results are extremely promising! The objective is to achieve an r squared\ngreater than 0.85 when we have access to at least two years of historical data.\nFrom the metrics, we can see that half of the buildings meet the performance criteria, \nwhile some of the other buildings have lower metrics. We can compare the length of the \ndataset to the accuracy to get an indication of how much the amount of training\ndata affects the performance of the model.\n\n```{r, echo = FALSE}\n\n# Find length of all datasets in years\nlenyrs <- c()\nnames <- c()\ntrain_lens <- c()\n\n# Iterate through all the files\nfor (file in filenames) {\n  # Create the training/testing features/labels\n  df <- read_data(file)\n  name <- unlist(strsplit(file, '-|_'))[2]\n  \n  leny <- nrow(df) / (60 / (as.numeric(df$timestamp[5] - df$timestamp[4])) * 24 * 365)\n  train_len <- nrow(df) - (60 / (as.numeric(df$timestamp[5] - df$timestamp[4])) * 24 * 180)\n  \n  lenyrs <- c(lenyrs, leny)\n  names <- c(names, name)\n  train_lens <- c(train_lens, train_len)\n}\n\n# Dataframe with lengths and r2 values\ndf_lens <- data.frame('building' = names, 'years' = lenyrs, 'points' = train_lens)\ndf_lens <- merge(df_lens, dplyr::filter(months_metrics, metric == 'r2'), by = 'building')\n\n# Plot r2 versus years of data with linear fit, indicate uncertainty\nggplot(df_lens, aes(x = years, y = value)) + geom_point(size = 2, color = 'firebrick') + \n  xlab('Years of data') + ylab('R2') + ggtitle('R2 vs Years of Data') + \n  theme_classic(12) + th + scale_y_continuous(breaks = seq(0.4, 1.0, 0.1)) + \n  geom_smooth(se = TRUE, method = 'lm') + geom_text(aes(y = value + 0.05, label = building))\n\n# Plot r2 versus number of datapoints with linear fit, indicate uncertainty\nggplot(df_lens, aes(x = points, y = value)) + geom_point(size = 2, color = 'firebrick') + \n  xlab('Number of Training Datapoints') + ylab('R2') + ggtitle('R2 vs Training Points') + \n  theme_classic(12) + th + scale_y_continuous(breaks = seq(0.4, 1.0, 0.1)) + \n  geom_smooth(se = TRUE, method = 'lm') + geom_text(aes(y = value + 0.05, label = building))\n```\n\nFrom the plots, we can clearly see there is a positive relationship between the \nnumber of training data points and the accuracy of the model. This relationship \nis entirely to be expected, because as the number of valid training data points increases, \nthe accuracy of any regressor (or classifier) will improve. A good article on this \ntopic was written by [Alon Halevy, Peter Norvig, and Fernando Pereira](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf), \nseminal leaders in the field of machine learning. \n\nThe random forest regression model is currently being tested against all datasets\navailable to EDIFES to get a better indicator of its ability. The performance \nwill be presented in the final report. \n\n# Conclusion\n\nIn this report, I evaluated a number of different modeling techniques \n(linear, svm, and random forest), implemented the most successful one for prediction, \nsimulated global warming, and assessed the prediction ability of the best model.\nModeling is a crucial part of the data analysis pipeline because it allows one to \ndetermine the strength of relationships between response and explanatory variables. \nThis can then be translated into real-world applications by concentrating on those \nvariables that have the largest effect on the response. Experiments can be run wherein \nthese features are slightly altered in order to test the model or try to achieve a\ndesired response. For the building energy problem, an ideal model would allow \nthe EDIFS team to alter factors such as the effective thermal resistance of a building \n(a function of the amount of insulation and quality of seals) or the HVAC schedule \nand observe the effects on building energy use. \n\nThree models were evaluated in this report for their ability to predict electricity\nconsumption from all weather and time variables: both uni- and multivariate \nlinear regression; a support vector machine using a radial basis kernel; and a random forest \ncomposed of 200 decision trees. On a random training/testing split, the random \nforest significantly outperformed the other two models in terms of RMSE, mape, and\nR-Squared. Therefore, the random forest was selected for further exploration. This \nexploration included a simulated modeling of a 2 degree Celsius \nincrease in temperature, and an attempt to predict energy consumption for six months\nusing known weather data. The explorations revealed both the \nstrengths of the random forest model as well as its limitations. Moving forward in the \nEDIFES project, I expect a random forest model could play a vital role in \nhelping us to understand and improve building energy consumption.\n\nFollowing are the 10 primary conclusions from this report:\n\n1. Temperature is the most highly correlated variable with daily energy consumption. \nHowever, a univariate linear model attempting to explain daily energy use from only temperature \nhas an R-Squared value below 0.20 for all buildings during the summer. \n  * A 1 degree Celsius increase in temperature can increase electricity consumption\n  by up to 300 kWh over the course of a day in an office building. \n  \n2. Including all of the weather variables in a linear model with energy as the \nresponse yielded slightly better predictions although the model was not\nsignificantly better than the simple model with only temperature.\n\n3. The three significant weather variables for predicting daily energy consumption in a \nlinear model are temperature, global horizontal irradiance (GHI), and global tilt irradiance (GTI). \n\n4. When trying to predict energy consumption for each 15-minute interval\nfrom all of the time and weather variables, the random forest significantly outperformed\na support vector machine regression and a linear regression. \n  * On a random training/testing split, the Random Forest Regression achieved\n  an average R-Squared of 0.968 and a mean average percentage error of 6.65%\n  \n5. The best Random Forest model uses the following hyperparameters: 200 trees in the forest,\nno maximum depth of the decision trees, a minimum of 1 sample per leaf, a minimum of \n2 samples to split a node, and a maximum of half of the features used for each tree.\n\n6. The most important variables for predicting energy consumption as ranked by the \nRandom Forest are: temperature, weekday or weekend, time of day in hours,\nbusiness day or non business day, timestamp, and diffuse horizontal irradiance.\nSeveral variables, including month and sun rise or \nsunset, were not significant for predicting energy consumption. \n\n7. A 2 degree Celsius increase in average temperature results in an average \nincrease in energy consumption by 1.568% for the eight buildings studied. Prediction\nwas carried out using the random forest with temperatures 2 degree higher\nthan in the training data. \n  * An increase of this size can result in more than $20000 increased electricity\n  costs over the course of one year for a single building.\n  * Two buildings showed a decrease in electricity consumption with an increase in temperature\n  \n8. A random forest regression model could be used to make estimates of the effect of \naltering one or several variables and finding the change in energy response. However,\nthis would have to be limited to days on which we have reliable \nweather information. \n  * It is possible to replay past days and alter one or more variables\n    in order to determine the effect on energy consumption\n  \n9. The Random Forest model might be capable of meeting the EDIFES project requirement\nof 0.85 r-squared when predicting six months of energy consumption. A study on all\nthe Project EDIFES buildings is ongoing that will determine the applicability of the method. \n  * Prediction was done on all eight buildings, with four passing the requirement\n  * There is a definite positive relationship between the number of training \n  datapoints and the prediction accuracy of the regression model. \n\n\n10. Overall, the problem of predicting energy consumption from weather and time\nvariables in 15-minute increments is a highly non-linear task. There are significant\ndifferences in energy consumption depending not only on the weather conditions, but the time\nof day, the day of the week, the season, and the building characteristics. Any supervised model\nthat can accurately predict the energy usage of a building must be\nnon-linear and highly flexible. \n\nI will see you in the final report! ",
    "created" : 1512824760302.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3058353906",
    "id" : "F367072A",
    "lastKnownWriteTime" : 1512918047,
    "last_content_update" : 1512918047224,
    "path" : "~/DSCI 451/17f-dsci351-451-wjk68/1-assignments/SemProj-451/building_energy_eda/reports/eda_report_three.Rmd",
    "project_path" : "reports/eda_report_three.Rmd",
    "properties" : {
        "last_setup_crc32" : "559F29AF7ad8d23b"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}