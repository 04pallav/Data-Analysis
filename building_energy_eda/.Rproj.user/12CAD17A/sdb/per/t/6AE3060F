{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Final Report for Building Energy EDA\"\nauthor:\n -  William Koehrsen, Case Western Reserve University\nkeywords: \"Energy Efficiency, Energy Audits, Commercial Buildings, Data Analysis\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\ngeometry: margin=1in\nfont: helvetica\nfontsize: 12pt\nabstract: |\n  The Energy Diagnostic Investigator for Efficiency Savings (EDIFES) Project is a \n  United States Department of Energy funded undertaking to develop a platform \n  for virtual building energy audits. Commercial buildings squander up to 30% of the\n  electricity they consume and these wastes are especially egregarious because they\n  can be addressed using existing technologies. The problem is not developing the \n  solutions, but rather identifying and correcting the inefficiencies. Traditionally,\n  this has been accomplished through in-person physical energy audits, a time and \n  cost-intensive operation with questionable economic benefits. By enabling virtual\n  energy audits that do not require ever setting foot ina building, Project EDIFES\n  will\n  reduce the time and cost to perform an audit will increasing accuracy, leading\n  to improved economic and environmental benefits. A vital first step in working on\n  any data science project is to gain an intuitive understanding of the data and \n  tools avaiable. This project was meant to serve as an introduction to the\n  building energy domain and to familiarize the author with the  techniques\n  commonly employed by EDIFES team members [1]. An explaratory data analysis was\n  carried out on eight Progressive Office Buildings located in six different\n  Koppen Geiger climate zones. An EDA is an open-ended process, and the primary\n  goals were to identify relationships and patterns within the energy data \n  that could be used by the EDIFES project to develop recommendations for \n  improving building efficiency. Over the course of the project, the objectives\n  focused on the correlations between weather conditions and energy consumption,\n  and subsequently, creating a machine learning model to predict the energy\n  consumption\n  from the explanatory varibles. This report details work and findings from both the \n  open-ended EDA and the focused modeling section of the project. Overall,\n  the temperature was the most strongly correlated weather variable with energy \n  consumption, followed by ghi, and the time of day in hours and the day of the week\n  were the most strongly correlated time variables with energy consumption. The \n  random forest regression algorithm was the most capable prediction algorithm \n  and is now being further developed for use in predicting six months of \n  energy consumption for all of the data available to Project EDIFES. An EDA was an \n  optimal introduction to the building energy problem and opened many potential \n  avenues for further investigation.\n  \n  \n  \n  \n  \n  \noutput: \n  pdf_document:\n    number_sections: true\n    toc: true\nurl_color: blue\n---\n\n<!--\n# Script Name: eda_final_report.Rmd\n# Purpose: Final report for DSCI 451 Semester Project\n# Authors: William Koehrsen\n# Author Affliation: EMAE and EMSE Department Case Western Reserve University\n# License: Creative Commons Attribution-ShareAlike 4.0 International License.\n##########\n# Latest Changelog Entries:\n# v0.00.01 - 12/07/17 - eda_final_report.Rmd - William Koehrsen began this rmd\n# v0.00.02 - 12/08/12 - eda_final_report.Rmd - William Koehrsen worked on introduction\n# v0.00.03 - 12/09/17 - eda_final_report.Rmd - William Koehrsen completed\nup through modeling\n# v0.00.04 - 12/10/17 - eda_final_report.Rmd - William Koehrsen finished first draft\n# v0.00.04 - 12/10/17 - eda_final_report.Rmd - William Koehrsen made initial edit\n# v0.00.05 - 12/12/17 - eda_final_report.Rmd - William Koehrsen completed final version \n##########\n\n\n# Rmd code goes below the comment marker!\n-->\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(cache = TRUE)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.align = 'center') \nknitr::opts_chunk$set(dev = 'pdf')\n```\n\n# Introduction\n\nThe Progressive Building Energy EDA was meant to serve as an introduction to the\nEDIFES problem domain. While not directly contributing to the project, this work \nhas identified relationships and modeling methods that will be of used to the team.\n\n## Problem Overview\n\nUp to 30% of the energy used by commercial buildings in the United States is\nwasted [2]. Nearly all waste is a result of preventable causes including\ninefficient heating, ventilation, and air conditioning (HVAC) scheduling,\nimproper insulation, exterior lights that are uncoordinated with natural\nlighting cycles, or oversized equipment. The barrier to fixing these\ninefficiencies is not technological, as solutions already exist, but rather\nidentification of savings opportunities. An energy audit is the accepted\nsolution to this problem and involves inspection and analysis of energy flows\ninto and out of a building [3]. Typical commercial in-person energy audits\nrequire sending a team of auditors to the physical building location to perform\na series of tests. These tests use a range of techniques and equipment, from\nblower checks of window and door seals to infrared camera scans of rooms to\ndetermine where a building is squandering electricity. Due to the human labor\nand equipment required, these physical energy audits carry a considerable cost,\nup to $15,000, and have a turn-around time of weeks or months [4]. Moreover, the\nrecommendations for the building owner can vary considerably depending on the\nauditing team. These factors combine to reduce the economic value of an energy\naudit. EDIFES, a joint project between Case Western Reserve University and the\nGreat Lakes Energy Institute, aims to address these limitations by\nperforming virtual energy audits that do not require setting foot in a building.\nTo reduce requirements on the building owner, she/he will only need to provide\nEDIFES with the square footage and location of the building. The electricity\nusage is obtained directly from the utility company, typically in 15-minute\nenergy consumption (in kWh) for at least one past year. The weather data\ncorresponding to each building is retrieved from solarGIS [6] and merged with\nthe energy information. The objective of the EDIFES project is a software\nplatform that will automatically receive electricity data and metadata, clean\nand analyze the data, and produce a human-understandable report with\nrecommendations for efficiency improvements. EDIFES will reduce the time and\ncost burden of conducting energy audits and consequently will increase the\neconomic benefit of this critical procedure.\n\n## Approach\n\nThe procedure for performing a virtual energy audit consists of three distinct steps [7]:\n\n1.\tObtain electricity and weather data in a clean and standard format\n2.\tAnalyze the data using statistical methods encoded in R functions\n3.\tReport results and develop recommendations for the building owner\n\nThis project was not directly concerned with the virtual energy audit procedure, \nbut was designed to be exploratory in nature with no fixed outcome. The basic\nprocedure followed in this EDA is as follows:\n\n1. Formulate initial exploratory questions that are expected to change over the \nproject.\n2. Determine the data that needs to be gathered to answer the questions.\nObtain, clean, and structure the data into a tidy format. \n3. Explore the data using basic quantitative summary statistics and visualizations.\n4. Determine trends and relationships within the data.\n5. Evaluate several models to capture the relationships identified.\n6. Select the best model and use it to answer questions.\n7. Challenge the model, assess the model using quantitative metrics, iterate\nas required.\n8. Interpret and report the results of the model. \n\nThroughout the report, the following meteorological definitions of summer and winter\napply\n\n* summer = June, July, August\n* winter = December, January, February\n\nThese \"seasons\" are not necessarily valid across the entire country but are\nused by the EDIFES team because no other definition has yet been developed. \nThis is an area of concern because clearly, the same definitions for the seasons\nare not applicable in all climate zones. \n\n## Exploratory Questions\n\nBased on the goals of the overall project and an initial exploration of the data,\nthe following set of questions was formulated at the beginning of the semester.\nIt was expected these would change over the course of the semester and \nadditional queries would develop  based on what the data could and could \nnot answer.\n\nIn the words of [John Tukey](https://en.wikipedia.org/wiki/John_Tukey) (a mathematician who developed the FFT and boxplot): \n\n> Data analysis, like experimentation, must be considered as an open-ended, highly interactive,\niterative process, whose actual steps are selected segments of a stubbily branching, \ntree-like pattern of possible actions.\n\n### Initial \n\n1. Which weather variables are most correlated with energy consumption and what is the\nphysical explanation behind this?\n2. Can the current day's weather data be used to predict tomorrow's energy consumption?\n3. Controlling for building size, which climate zone is the most energy intensive?\n4. Are these buildings \"good\" in terms of energy efficiency? What does \"good\" mean in this context \n(i.e. how can it be quantified)?\n5. Can the next 10 markers accurately characterize these buildings?\n6. Based on the answers to the previous 5 questions, are there concrete recommendations \nfor building managers to reduce energy use?\n\n### Final\n\nBy the end of the semester, the focus had narrowed to one main question \nwith two parts:\n\n__Which weather and time conditions are most correlated with energy use,__\n__and is it possible to build a model to predict energy use from the__\n__explanatory variables?__\n\nOver the course of the semester, it was clear that several of the initial questions\ncould not be satisfactorily answered, \nbut the final question was thoroughly addressed.\n\n# Data Science Methods\n\nA number of different data science techniques and concept were covered during\nthe course of the EDA. These ranged from basic statistical concepts to machine \nlearning algorithms. \n\n## Statistical Concepts\n\nStatistical concepts covered in this project are summarized below:\n\n* Pearson Correlation Coefficient: ranges from -1 to +1 and demonstrates the \nstrength and direction of a linear relationship between two variables. -1 indicates\na perfectly linear negative relationship and +1 indicates a perfectly positive\nlinear relationship\n* R-Squared: The squared of the Pearson Correlation Coefficient. Indicates the \npercentage of variation in the response variable explained by the explanatory\nvariables in the model. This can be used to assess the performance of a model\nin capturing relationships within the data.\n* Box-Cox Power Transformation: A method for determining the best transformation\nto normalize data. This applies a number of power transformations to the data\nand returns the one that results in the most normal distribution.\n* Central Limit Theorem: This theorem states that for a non-skewed sample \ndistribution, the distribution approaches normal as the sample size increases.\nA general heuristic is that 30 samples is the minimum for satisfying the theorem.\nThis means that it is not possible to make statistically significant comparisons\nbetween the eight office buildings, and an additional source of data\nwas used to overcome this limitation. \n\n## Machine Learning Modeling  \n\nThe challenge of predicting energy consumption from weather and time data is\na supervised regression task. It is supervised because the actual labels, in this \ncase the energy consumption for each 15-minute interval, is known, and it is\na regression task because the labels are continuous values. During training,\nthe labels and features (explanatory variables) are given to the model \nso it can learn a mapping between the features and the labels.In testing, \nthe model is only given the features and must predict the value of the labels. \nThe predictions can then be compared to the known values (in this case the known\nenergy consumption) to assess the performance of the model.\nThere are numerous modeling approaches for supervised regression tasks that can \nbe implemented in a number of programming languages. \nThese range in complexity from linear models with simple equations and \nunderstandable variable weights, to deep neural networks which are \nessentially black boxes that accept data as an input, perform a sequence\nof mathematical operations, and produce an (often correct) answer with no\nhuman understandable interpretation. A major concentration of this project \nwas intrepretability in addition to accuracy, and models were chosen for the \nbest combination of these two features. The three algorithms selected were\ndesigned to cover the range of complexity and interpretability in models,\nstarting with the simplest, Linear Regression, proceeding to the ensemble Random\nForest method, and finally moving on to the Support Vector Machine Regressor.\n\nThe Linear Regression has no hyperparameters to tune, the Random Forest was\nbuilt with the defaults except the number of decision trees was set to two\nhundred, and the Support Vector Machine used all default hyperparameters and the\nradial basis kernel. To compare the algorithms, a random train-test split was\nused (with the same features and labels used for all models). The objective was\nto select the best performer on the random split and implement this method for\npredicting six months of energy consumption. All three algorithms were built\nusing the Scikit-Learn library in Python [24] while the data was preprocessed in\nR. Transferring the data between R and Python is relatively simple with the\nfeather library that saves and loads dataframes in R and pandas dataframes in\nPython. \n\nThe basic steps for evaluating the algorithms on a random\ntraining/testing split are as follows: \n\n1. Structure the data into an appropriate format, and separate the features and labels. \n2. Subset the data into a training\nand testing set using a 0.75/0.25 split \n3. Train each model on the same training\nset with the model fed both the features and the labels. \n4. Use the trained model to predict the testing labels from the testing features. \n5. Compute performance metrics with the known testing labels and predicted values. \n6. Make comparisons between algorithms for interpretability and accuracy. \n\nData preparation involved the following four steps\n\n* One-hot encoding of categorical variables\n* Transformation of cyclical variables into a sinusoidal representation\n* Normalize variables to have zero mean and unit variance\n* Separate into random training and testing features and label\n\nA brief overview of the three machine learning algorithms is presented below:\n\n### Single and Multivariate Linear Regression\n\nThe simplest model and the ideal place to start  is linear regression which aims\nto explain the variance in the response variable by a weighted linear addition\nof the explanatory variables. Linear Regression can use a single explanatory\n(independent) variable, or it can use many. Taking a power transformation of the\nexplanatory variables in order to explain the dependent variable is still a\nlinear model because the response model is still a linear addition of\ncoefficients multiplied by the respective independent variable. The general\nequation for a linear model is\n\n$$y = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n$$\nwhere $w_0$ is the intercept, $x_n$ represents the explanatory variables, $w_n$\nis the weight assigned to each explanatory variable, and $y$ is the response\nvariable. In the case of the Building Energy Dataset, y is the energy\nconsumption, x is the weather or time features, and w is the weight assigned to\neach feature. The weight in a linear model represents the slope of the\nrelationship, or how much a change in the x variable affects the y variable. A\ngreat aspect of linear modeling is that the impact of change in one independent\nvariable can be directly observed and predicted.\n\n### Random Forest\n\nTo understand the powerful random forest, one must first grasp the concept of a\ndecision tree. The best way to describe a single decision tree is as a flowchart\nof questions, which leads to a classification/prediction. Each question (known\nas a node) has a yes/no answer based on the value of a particular explanatory\nvariables. The two answers form branches leading away from the node. Eventually,\nthe tree terminates in a node with a classification or prediction, which is\ncalled a leaf node. A single decision tree can be arbitrarily large and deep\ndepending on the number of features and the number of classes. They are adept at\nboth classification and regression and can learn a non-linear decision boundary.\nHowever, a single decision tree is prone to overfitting, especially as the depth\nof the tree increases because the decision tree is flexible leading to a\ntendency to memorize the training data (that is, it has a high variance). To\nsolve this problem, ensembles of decision trees are combined into a powerful\nclassifier known as a random forest [17]. Each tree in the forest is trained on\na randomly chosen subset of the training data (either with replacement, called\nbootstrapping, or without) and on a subset of the features. This increases\nvariability between trees making the overall forest more robust and less prone\nto overfitting. In order to make predictions, the random forest passes the\nexplanatory variables (features) of an observation to all trees and takes an\naverage of the votes of each tree (known as bagging). The random forest can also\nweight the votes of each tree with respect to the confidence the tree has in its\nprediction. Overall, the random forest is fast, relatively simple, has a\nmoderate level of interpretability, and performs extremely well on both\nclassification and regression tasks. There are a number of hyperparameters (best\nthought of as settings for the algorithm) that must be specified for the forest\nbefore training with the most important the number of trees in the forest, the\nnumber of features considered by each tree, the depth of the tree, and the\nminimum number of observations permitted at each leaf node of the tree. These\ncan be selected by training many models with varying hyperparameters and\nselecting the combination that performs best on cross-validation or a testing\nset. A random forest performs implicit feature selection and can return relative\nfeature importances which can be interpreted in the domain of the problem to\ndetermine the most useful variables for predicting the response. A model of a\nrandom forest that predicts energy consumption based on the time and weather\nconditions is presented below.\n\n```{r, echo = FALSE}\nknitr::include_graphics('../treevis_verysmall.png')\n```\n\n### Support Vector Machine\n\nA support vector machine (SVM) regressor is the most complicated and the least\ninterpretable of the models explored in this report. SVMs can be used for both\nclassification and regression and operate based on finding a hyperplane to\nseparate classes. The concept is that any decision boundary becomes linear in a\nhigh enough dimensional space [14]. For example, a linear decision boundary in\nthree-dimensional space is a plane. The SVM projects the features of the\nobservations into a higher dimensional space using a kernel, or a transformation\nof the data. The model then finds the plane that best separates the data by\nmaximizing the margin, the minimum distance between the nearest member of each\nclass and the decision boundary. The support vectors in the name of the\nalgorithm refer to the points closest to the decision boundary, called the\nsupport, which have the greatest influence on the position of the hyperplane.\nSVM regressors work by fitting a non-parametric regression model to the data and\ntrying to minimize the distance between the model and all training instances.\nSVM models are more complex than either a linear regressor or a random forest\nregressor and have almost no interpretability. The transformation of the\nfeatures into a higher-dimensional space using a kernel removes all physical\nrepresentation of the features. SVM models are black boxes but have high\naccuracy on small datasets with limited amounts of noise. The support vector\nmachine also takes much longer to train than either of the other models. That\nbeing said, this approach demonstrated successful results in the literature and\nwas therefore investigated [15] [16]. An example of a support vector machine for\nclassification with several different kernels is shown below\n\n```{r, echo = FALSE}\nknitr::include_graphics('../svc_kernel.png')\n```\n\nThe RBF or Radial Basis Kernel, is the most popular kernel in use in the literature\nand is the default in most implementations. \n\n# Exploratory Data Analysis\n\nThe driving goals behind the EDA were to determine the trends and patterns within\nthe electricity consumption data, and to find the most significant relationships. \nThe outcome of the EDA was not fixed, but the results would be used to inform \nobjectives and inputs to the modeling section. \n\n## Trends and Patterns\n\nThe EDA quickly revealed building energy consumption exhibits daily, weekly, and \nseasonal repeating patterns in addition to overall trends. Moreover, these patterns\ndiffer significantly between buildings in different climate zones and different \nbuilding types (for example, retail buildings that are open 7 days a week \ncompared to office building operating only during the week). The patterns were\nanalyzed at different time scales and buildings were compared both to themselves in \ndifferent seasons, and to other buildings in different climate zones. \n\n### Long Term Trends\n\nTo get a feel for the overall structure of the data, it is best to graph the entire\ntime series. The following graphs show several different buildings with business\ndays separated from the non-business days. \n\n```{r}\n# Entire time-series for three buildings\nknitr::include_graphics('kansas_full.png')\nknitr::include_graphics('las_vegas_full.png')\nknitr::include_graphics('phoenix_full.png')\n\n```\n\nThere are a number of noticeable trends in these plots. For all the buildings,\nenergy consumption increases during the the summer which agrees with domain knowledge. The largest source of office building energy consumption tends to be \nthe heating, ventilation, and air conditioning (HVAC) system, which tends to \nsee more use during the summer months for the southern climates. Kansas City, \nLas Vegas, and Phoenix experience hot summers and hence will see an increase in \nenergy consumption due to air conditioning use. The buildings in Las Vegas and \nPhoenix exhibit a decrease in energy consumption during the winter because these\ncities typically experience mild winters which do not necessitate \nenergy-intensive heating. Meanwhile, the building in Kansas City does not exhibit\nthe same large decrease during the summer. Energy consumption for this building\nhas two peaks (it is bi-modal), with one occurring during late summer, and the other\noccurring during late winter. \n\n### Monthly Trends\n\nA great plot for understanding the location and spread of data is the boxplot. \nA boxplot shows the median, the interquartile range, and any outliers in a dataset.\nThe following graphs show boxplots with the average energy consumption per month\nfor the same three buildings. These boxplots are computed using only business days.\n\n```{r}\n# Graphs of monthly consumption as boxplots\nknitr::include_graphics('kansas_month.png')\nknitr::include_graphics('las_vegas_month.png')\nknitr::include_graphics('phoenix_month.png')\n\n```\n\nThe same yearly behavior is observed as before, with an increase during summer\nfor the Las Vegas and Phoenix buildings, and two yearly peaks in consumption\nfor the Kansas building. The Kansas building has significantly more outlying \npoints during the spring months indicating that daily energy consumption can vary\nsignificantly during these months. This is not unexpected as some springs\nmay be much colder than others, necessitating higher heating use.\n\n### Weekly Trends\n\nFinally, the last two patterns in the data are those over the course of a week\nand throughout the day. The graphs are colored by season to different the \npatterns that occur throughout the year. The energy consumption is in kWh for\neach 15 minute interval, averaged over all the points in a specific day \nduring a specific season.\n\n```{r}\n# Graphs for each week, colored by season, faceted by day of week\nknitr::include_graphics('kansas_week.png')\nknitr::include_graphics('las_vegas_week.png')\nknitr::include_graphics('phoenix_week.png')\n```\n\nThese graphs show trends by week, over the course of a day, and in different \nseasons. It is difficult to draw general conclusions because all of the buildings\ndisplay different behavior. However, it is clear that energy consumption is higher\nduring the week as expected for office buildings. The energy use typically rises\nin the morning which would correspond to the HVAC starting and workers arriving\nat the building. The energy consumption remains high during the work day, and then\ndrops significantly overnight. The energy consumption never drops to zero\nbecause all buildings still have systems that must be kept running constantly. This \nis referred to as the baseload of a building and is a target for energy use\nreduction. Energy use in noticeably higher in the summer for the Phoenix and \nLas Vegas buildings and also slightly higher during the summer for the Kansas \nbuilding. Spring and Fall generally exhibit lower energy consumption than winter and \nsummer, except for the Kansas building where energy consumption is \nsecond greatest during the fall. The weekend patterns are very interesting\nas the energy consumption is moderate during the morning and afternoon, in line\nwith that observed during the day, but then drops precipitously during the \nafternoon. This is intriguing because it suggests certain systems are shut off\nduring the afternoon on the weekends but then are turned on again during\nthe morning and afternoon. Overall, there are numerous conclusions to draw\nabout a single building from these plots, and they will be a useful tool to the\nEDIFES team when analyzing the energy profile of a building.\n\n## Weather Correlations\n\nThe second major aspect of the exploratory data analysis was discovering the \nsignificant correlations between weather conditions and energy consumption. \nBased on these relationships, it should be possible to inform a building owner\nahead of time based on the weather forecast how much energy they can expect to use \nand how to potentially mitigate energy use based on the weather. \n\nThe best approach for determining weather correlations is to calculated them \nseparately during the winter and summer. Depending on the climate zone\nand the type of heating used by the building, the direction and strength of \nweather correlations vary greatly. \n\nLooking at the numerical correlations is useful, but a quicker way to gauge the \nrelative magnitudes between buildings is with heatmaps. These show stronger \ncorrelations as more vivid colors and can express large amounts of information\nin a compact form. These heatmaps show the Pearson Correlation Coefficient\nbetween the respective weather variable and energy consumption during \neach 15-minute interval. \n\n```{r}\n# Summer and Winter correlation heatmaps\nknitr::include_graphics('summer_heatmap.png')\nknitr::include_graphics('winter_heatmap.png')\n\n```\n\nThese visualization contain a significant amount of information. The major \nconclusion for the summer is that temperature tends to be positively\ncorrelated with energy consumption as does ghi, dif, and gti. The exception \nto this statement is the building in Portland, Oregon, which exhibits the exact\nopposite correlations as the other buildings. This was considered to be anomalous,\nand the building owners were contacted for an explanation, but none was provided.\nIt is possible that the time stamp on the electricity meter is systematically wrong,\nor the measurements could be strange but correct. This building is the only\nbuilding location in a relatively cold climate, which could explain the \ndifference in behavior. The knowledge that temperature and the three irradiance \nvariables are positively correlated with energy consumption (ignoring the Portland building for now)\ncould inform recommendations to a building owner. These might include:\n\n* Add additional insulation to keep the cool air in during the summer\n* Install double-paned windows with adequate sealing\n* Cover windows in direct sunlight during the day\n* Schedule employees to work earlier in the morning and leave during the afternoon\nwhen the temperature peaks.\n\nThis last recommendation may appear drastic, but it could be necessary as \nthe efforts to reduce energy consumption become more urgent. However, the other\nrecommendations are much simpler to implement and can result in significant\nreductions in energy use with no disruption to productivity. \n\nThe majority of the winter correlations are smaller in magnitude and not consistent\nacross the buildings. In particular, the temperature correlation varies \nsignificantly as it is positive in some buildings and negative for others.\nOne potential explanation is the typical heating\nin use in the climate zone. If a building has gas heat, then as the temperature \ndecreases, it would not be expected that the electricity consumption would increase\nand hence the temperature - energy correlation will be small or positive\nduring the winter.\nA building with electrical heating will need to use more electricity as the \ntemperature decreases, and hence the temperature - energy correlation will be \nnegative. That is, as the temperature decreases, the energy use increases. The \nPortland building again stands out, although it is similar to the Kansas City\nbuilding which could support the hypothesis that these buildings are different\nbecause are located in colder parts of the country than the other six buildings. \nThere is considerable additional analysis that can be performed on these correlation\nheatmaps, and they hold potential for informing decisions with real-world benefits.\n\nAfter performing this analysis on the eight Progressive buildings, a similar analysis\nof weather correlations with energy consumption was done on all 750+ building\ndatasets that EDIFES currently owns. The results are as follows where the numbers\nagain correspond to the Pearson Correlation Coefficient. The climate zones \nare the Koppen Geiger Climate Zones. The branching trees on the top and left \nside of the heatmap are dendrograms created by a hierarchical clustering. \nThe clustering groups together similar climate zones and similar weather conditions\nby the magnitude of the correlation coefficient. Using these dendrograms could\nprovide a method for comparing buildings in different climate zones. \n\n```{r}\n# Correlation heatmaps for all buildings by climate zone\nknitr::include_graphics('summer_kgcz.png')\nknitr::include_graphics('winter_kgcz.png')\n\n\n```\n\nIn general, temperature, gti and ghi are positively correlated with energy \nconsumption during the summer and relative humidity is negatively correlated\nwith energy use. \nHowever, during the winter, there are significant differences between the temperature\ncorrelation by climate zone, with some zones exhibiting a negative correlation and \nothers a positive correlation.  The \nmajority of the winter correlations are small in magnitude as was\nobserved for the eight Progressive office buildings. The smaller magnitude of these\ncoefficients might be due to average across many buildings in each climate zone. \nFor example, in a climate zone with two buildings, if one building has a positive \ncorrelation between temperature and energy during the winter and the other has a \nnegative correlation, averaging these two out may lead to the conclusion that \ntemperature has no correlation with energy use in that climate zone during the \nwinter. A better approach may be to examine each individual building as was done\nin the EDA and then compare the building to the average for the climate zone. \n\n# Modeling and Prediction\n\n## Linear Modeling\n\nModeling started off with a basic univariate linear model. The objective was\nto predict the average daily energy consumption from the average daily\ntemperature. Due to the differences between buildings and seasons, a different\nmodel was created for each building and season. Temperature was selected\nas the single variable because it displayed the highest correlation with \nenergy consumption. Following is a summary of all the linear models. Reported\nin the table is the slope of the temperature with respect to the energy consumption\nand the r-squared for each model during the summer and winter.\n\n```{r}\nload('all_temp_slopes')\n# All slopes of linear model\nknitr::kable(all_temp_slopes[1:8, ], caption = 'Summer and Winter Temp Linear Model Stats')\n```\n\n\nWhile a linear model cannot capture complex relationships, it does have a high \nlevel of interpretability because the features are not transformed. The slope\nof this model represents the daily change in kWh of energy use for a 1 degree Celsius\ndaily increase in temperature. Therefore, for the building in Sacramento, a 1 \ndegree warmer day during the summer will results in 308 kWh more electricity use,\nand at the national average energy price of \\$0.104 / kWh, \nthat represents \\$30 per day. During the winter, some buildings will observe \na decreased in electricity usage with a decrease in temperature, while other \nbuildings will have an increase in electricity usage with an increase in temperature.\nThe results also show that this model is not powerful. The summer r-squared is less\nthan 0.2 for all buildings indicating that the model can explain less than 20% \nof the observed variability in energy consumption based only on the temperature. \nThe winter r-squared values are similarly small. This indicates that a more complex\nmodel is needed, or more variables should be included in the linear model.\nAn example of the univariate linear model plotted on the actual summer data is\nbelow for the first building in Phoenix with a summer slope of 21.16. The points\nin red are the high leverage points, or those with the greatest distance\nfrom the main cluster of points. These points have a considerable influence on the \nslope of the model as can be seen in the contrasting lines representing the model\nwithout the high leverage points, and the model with only the high leverage points.\n\n```{r}\n# Linear model with high leverage points\nknitr::include_graphics('univariate_linear_model.png')\n```\n\nBefore moving on to more complex models, a linear model was created to predict\ndaily average energy use using all the weather variables. The energy use is not\ndependent on only temperature, and a model with more variables might be able\nto account for more of the variability in the average daily energy use. The following\ntable shows the r-squared values for a linear model created between temperature\nand all of the weather conditions.\n\n```{r}\nload('weather_r2')\n# R-Squared for average daily energy use models with all weather variables\nknitr::kable(weather_r2, caption = 'Weather Linear Model R-squared')\n```\n\nThe measured r-squared values are not significantly better than those \nproduced by temperature alone. An Analysis of Variance (ANOVA) test showed that \nthe model with all weather variables did not exhibit a statistically significant \nimprovement with alpha = 0.05 compared to the model with only temperature. Predicting\ndaily averages accurately is not possible because there are significant \npatterns that occur throughout the day. Averaging over an entire daily essentially\nreduces the amount of data available by a factor of 96, the number of daily\nobservations of weather and temperature. Therefore, in order to create a more \naccurate model, the model should predict the energy consumption for each and \nevery 15-minute interval from the corresponding time and weather conditions. \nTo find a daily total, these predictions can then be summed up over the 96\nintervals, or a more accurate approach to assessing the predictions is to compare\nthem to the known energy use for each 15-minute period. By making predictions every\n15 minutes, and using all of the time variables in addition to the weather variables,\nthe accuracy of the model and the percentage of the variation explained is \nexpected to be significantly greater. \n\n## Machine Learning Prediction\n\nThe daily averaging method with a linear model proved inadequate, and thus there\nis a need for more complex models operating at a higher granularity to capture all\nof the relationships within the energy information. Three models were selected\nfor comparison based on results reported in the literature and in order to cover\na range of complexity. A linear model, random forest, and support vector machine \nwere all evaluated for regression capabilities. The models were compared against\none another using a random 0.70/0.30 training/testing split of the data. In \nhindsight, comparing the models using the random split of data was not the optimal\nchoice because they were being evaluated for predictive abilities. However, the \nrandom split of the data should be adequate for comparing the algorithms because\nit still allows for relative evaluations. A model that cannot perform well at \nestimating the electricity consumption on a random split of the data also will not\nbe accurate at predicting months worth of electricity data into the future. The \nobjective of the model comparison phase was to determine the most promising model \nthat would then be optimized for the task.\n\n\n### Data Preparation \n\nIn order to compare models, they must be trained and tested on the same dataset. \nPreparation of the data was completed in R, and the results were then saved\nas feather files for loading into Python. The algorithms themselves were \nimplemented using the Skicit-learn library in Python because of the speed and \nincreased training control. The results were then saved as feather files and \nanalyzed using R because of the graphing capabilities. \nData preparation consisted of the following steps:\n\n__One-hot Encoding__\n\nOne hot encoding is a tricky concept at first that is best illustrated with an example.\nThe goal is to convert categorical variables into numeric variables without creating\nan arbitrary ordering. \n\nThe process takes this:\n\n| Day of Week |\n|-------------|\n| Monday      |\n| Tuesday     |\n| Wednesday   | \n\n\nand converts it into this:\n\n| Monday | Tuesday | Wednesday |\n|--------|---------|-----------|\n| 1      | 0       | 0         |\n| 0      | 1       | 0         |\n| 0      | 0       | 1         |\n\nEach value for each categorical variable is represented as either a 0 or 1.One-hot encoding is \nrequired because machine learning models do not know how to handle categorical data, and simply\nmapping the values to numbers imposes a random valuing of the feature values based on order, \nwhich may not be appropriate. \n\n__Cyclical variable transformation__ is crucial because some trends, such as months in\na year will not be properly represented by 1-12 numbers. Month 12 is closer to month\n1 than is month 5, but this is falsely depicted by using 1-12. Transforming the variables\ninto cosine and sine components creates the desired relationship between the \nvariables. This is done by using the equation for a sinusoidal \n$$y(t) = sin(2 * \\pi * f * t)$$   In the case of months, the frequency is 1/12 \nbecause the pattern repeats every twelve months (for daily time in hours, the frequency is 1/24). \nThe conversion for months is \ntherefore $$y(m) = sin(2 * \\pi * m / 12)$$ and $$y(m) = cos(2 * \\pi * m/12)$$. \n\nThe result is that month 12 is now represented as occurring closest to month 1 and month 11. \n\n__Normalization__ either means subtracting the \nmean and dividing by the standard deviation or \nsubtracting the minimum value and dividing by the maximum minus\nthe minimum. In the first approach, each feature column will\nhave 0 mean and unit (1) variance. The second approach scales each feature value to between 0 and 1. This step is necessary to remove any\ndisproportionate representations because of the varying units used in variables.\nIn other words, a variable with units of millimeters might have a larger weight attached\nto it than a unit of meters simply because of different units. Normalization \novercomes the problem of differing unit scales.\n\nFinally, the dataset must be split into __training and testing sets__. During training, the \nmodel is allowed to see the answers in order to learn the relationships (if any)\nbetween the explanatory and response variables. When testing, the model is asked to \nmake predictions for a set of features it has not seen before. The targets for these\nfeatures are known and therefore the performance metrics can be computed based \non the discrepancy between the known target values and the predictions.\n\n### Metrics \n\nNot only must the models be trained and tested on the same data, they must be \nevaluated using the same performance metrics. The following set of metrics was\nused for comparison:\n\n1. __rmse__: root mean square error; the square root of the sum of the squared deviations \ndivided by the number of observations. This is a useful metric because it has the same units\n(kWh) as the true value so it can serve as a measure of how many kWh the average\nestimate is from the known true target. \n\n2. __r-squared__: the percentage of the variation in the dependent variable explained \nby the independent variables in the model.\n\n3. __MAPE__: mean average percentage error: the absolute value of the deviation for each \nprediction divided by the true value and multiplied by 100 to convert to a percentage. This metric \nindicates the percentage error \nin the prediction and can be better than the rmse because it takes into account the relative \nmagnitude of the target variable.\n\n## Model Comparison\n\nThe results of the model comparison are shown in the following graphs.\n\n* lr = linear regression\n* svm = support vector machine regression\n* rf = random forest regression\n\n```{r}\n# Comparison of models on rmse, r-squared, mape\nknitr::include_graphics('rmse_comparison.png')\nknitr::include_graphics('rsquared_comparison.png')\nknitr::include_graphics('mape_comparison.png')\n```\n\nThe random forest regression model is the clear winner on all metrics.\nThe linear regression is not able to capture the relationship between\nthe inputs, the time and weather conditions, and the outputs, the energy consumption.\nThe support vector machine does considerably better, but it does not match the \nperformance of the random forest. Moreover, the runtime of the random forest was\nmuch lower than that for the support vector machine, and the svm does not have\ninterpretability because the features are transformed into a higher dimension\nusing a kernel (radial basis kernel in this implementation). The random forest\nis relatively simple to understand, has good interpretability because it returns\nthe relative importance of features, is quick to train and make predictions, and\nhas the best performance of all the models. Therefore, the random forest model \nwas chosen for refinement and testing with predictions of six months of\nenergy use. \n\n## Random Forest Validation and Implementation\n\nOnce the random forest had been selected, the next step was to optimize the \nhyperparameters for the particular problem. In contrast to model _parameters_ which \nare learning by the model during training, the _hyperparameters_ must be set by the \nprogrammer ahead of training. One way to think of these is as model settings that \ncan be turned back and forth until the model performs up to standards. \nIn Skicit-learn, models are built with a sensible\nset of default hyperparameters, but there is not guarantee these are optimal and only\na rigorous evaluation routine can determine the best set. \n\nHyperparameter tuning requires creating many models with different options and \nthen evaluating each using cross-validation to determine which set of configuration\noptions performs optimally. This can be done automatically in both R and Python and \nthe method used for this project was grid search with cross-validation using\nSkicit-learn in Python. The following set of hyperparameters achieved the highest \nmetrics on a random training/testing split (again, perhaps not the best choice for\nthis problem).\n\n* 200 decision trees in the forest\n* No maximum depth for each tree (trees will be grown until all predictions are made)\n* A minimum of 1 sample per leaf node\n* A maximum fraction of 50% of the features evaluated for each tree\n* A minimum number of 2 samples to split a node\n* A minimum impurity decrease of 0.0 to split a node\n\nThese hyperparameters resulted in an increase in performance of the algorithm\nwithout any noticeable increase in runtime. The following graph shows the \nroot-mean squared error comparison of the baseline to optimized model.\n\n```{r}\n# Baseline with Optimized RMSE comparison\nknitr::include_graphics('opt_base__rmse_comparison.png')\n```\n\nThe optimized model performs the baseline model in rmse for all buildings. \n\nThe next step was to challenge the random forest by predicting six months of energy\nconsumption. Six months was selected to correspond to the ARPE-E milestone\nto develop a predictive model capable of an adjusted r-squared of 0.85 \nwhen predicting six months worth of energy use. To implement this test,\nthe dataset was split into training and testing with the final six months\nused for testing and the first part of the data for training. During testing, the \nmodel would be asked to predict the six months from the recorded\nweather conditions and time. The data was again prepared in R, the algorithm\nimplemented in Python, and the results analyzed in R. \n\nThe prediction metrics are presented below in graphical form.\n\n```{r}\n# Prediction metrics for random forest\nknitr::include_graphics('prediction_metrics.png')\n\n```\n\nFour of the eight buildings meet the r-squared requirement. \nThe average performance metrics were a mape of %21.24, an r-squared of 0.77, and\nan rmse of 3.83 kWh. Although these metrics are not outstanding, they represent\na decent start towards meeting the project modeling requirement. Predicting six \nmonths of energy use is difficult, especially with some datasets not quite one\nyear in length. The random forest model is a promising method for predicting energy\nconsumption, but there is also a need to try additional methods. Another method \ncurrently used by the team is the diffusion indices method. One method that deserves\nmore exploration is a general additive model, in which the time-series is modeled\nas sum of patterns on different time scales (days, weeks, months, years), as\nwell as an overall trend (increasing or decreasing with time). The prophet package\n[32] in R provides a simple implementation although this is based on daily \nobservations and will require adjustment for more frequent observations.\nOverall, the prediction testing showed the random forest is capable of achieving\nrequired performance when predicting six months of energy consumption. Further \noptimization of the hyperparameters and feature engineering will improve the rf\nand it can be combined with ensemble methods to develop a robust prediction model.\n\n# Discussion\n\nThe Progressive office building EDA provided the ideal opportunity to explore the problem\ndomain. Many of the findings, while not entirely novel [7] [8], were surprising\nto the author. All of these could have been directly told to the author, but it\nis a more memorable exercise to have to write the code and make the\ndiscoveries firsthand rather than hearing them passively. Moreover, it is\nreassuring when the conclusions reached by an independent team member agree with\nthose established by the rest of the team. While disagreements drive progress,\nsimilar results confirm the methods in use and allow for the establishment of\nbase truths from which to work. The major finding was the number of different\npatterns that can be ascertained in the energy data. Each building exhibits\ndifferent trends depending on the time of day, day of the week, \nthe season of the year, and even between years if a retrofit to improve efficiency \nhas been performed.\nMoreover, once the office buildings had been thoroughly explored, a set of\ncoffee shops was examined and found to have entirely different patterns!\nClearly, the process of modeling and eventually forecasting energy consumption\nwill require sophisticated methods that can separate each of the trends [27]\n[28]. Additional significant findings from the EDA were weather and energy\ncorrelations that show temperature and global horizontal irradiance (GHI) are\nthe most highly correlated variables with energy use during the summer. These\nresults suggest several methods for reducing energy consumption including\nincreasing insulation, installing shades over windows in direct sunlight,\nproperly sealing windows and doors, and positioning buildings to use the sun as\na natural source of heating in the winter without absorbing excess heat in the\nsummer [29]. The EDA also showed the interpretations that can be drawn from a\nlinear model as well as the limitations linear models have because they are not\nflexible enough to capture intricate trends. One significant aspect of the EDA\nthat was unsuccessful was the attempt to normalize buildings across climate\nzones, building types, and building sizes. There were not enough buildings to\nmake statistically significant comparisons, but it might be possible\nwith the entire set of buildings available in HBase.\n\nThe modeling section of the report focused on two parts: an explanatory part\nin which the reason behind energy consumption was investigated, and a predictive\npart, in which energy consumption was predicted from a set of features. The final\nrandom forest model can be used for both explanation, in terms of feature importances,\nand prediction. \n\nExample predictions from the Random Forest are presented in the following figures:\n\n```{r}\n# Full Predictions for six months\nknitr::include_graphics('predictions_full.png')\n# Predictions for a week\nknitr::include_graphics('predictions_week.png')\n```\nThe first image shows the entire dataset and the predicted portion \nwhich aligns closely to the actual values. The second image shows a typical\nweek of predictions and how the random forest is able to accurately\ncapture all of the different patterns within the data. For this building,\nthe mean average percentage error was 15.72%, which is acceptable but will\nneed to be improved.\n\n## Interpreting Model\n\nTo get a sense of why the random forest makes a certain prediction, it can \nbe helpful to look at the feature importances. These are not useful so much in\nabsolute terms as in relative terms to rank features. The actual meaning\nbehind the importances is the reduction in uncertainty due to splitting a node\non the given feature. \n\nThe following graphs show first the feature importances for each individual\nbuilding, and then averaged over all the buildings. In total, after feature engineering,\nthere are 24 different features used by the random forest.\n\n```{r}\n# All importances for each building\nknitr::include_graphics('all_importances.png')\n# Importances averaged over all buildings\nknitr::include_graphics('average_importances.png')\n```\n\nTemperature is the single most important factor for determining the energy\nconsumption at any 15-minute interval. This is in agreement with the correlation\ncoefficients and with the linear modeling. Moreover, dif, gti, and ghi are the \nnext highest importance weather variables, in agreement with the correlation\ncoefficients. The day of the week and the time of the day have high importance,\nwhich makes sense given the domain because the weekends have considerably lower\nconsumption than during the week, and the consumption also varies greatly over the\ncourse of a typical day. The cosine transformation of the time of day has a higher\nimportance than the raw time itself, which validates the cyclical feature \ntransformation performed during data preparation. There are several variables\nwith very low importance, and these could likely be removed from the model without\nresulting in a performance decrease. The random forest performs implicit feature\nselection, and it will learn these are not useful, but removing them could help \nother methods that would only be \"confused\" by these unimportant features. \nMost of the features with low importance have low variance, meaning they fall\nwithin a narrow range, or take on the same value for most observations. \nIt is possible to remove features with low variance during data preparation\nusing preprocessing in the caret package. \n\nAlthough the random forest does not return weights associated with the features \nwhich means that the effect of a unit change in a feature cannot be assessed as in\nthe linear models, the random forest can still be used to observe the effects\nof altering a variable. Experiments with altering variables can be performed using\nthe random forest in order to assess effects of various changes in weather conditions.\n\n## Global Warming Experiment\n\nCurrent predictions indicate the global surface temperature will increase\nbetween 1 C and 4 C by 2100 as the result of human-caused climate change [25]\n[26]. In order to assess these effects on future energy use consumption, a\nreasonable increase of 2 C can be modeled using the random forest. This is done\nby training the model on the entire historical data available, and then creating\na testing set by keeping all of the explanatory variables the same except for\nthe temperature, which will be increased in every interval by 2 C reflecting a\nmean increase in line with current forecast for future warming. In this\nsituation, there are no correct values to compare the predictions to, and this\nis purely an experiment to observe the effects. The economic effects in terms of\nannual energy use costs are demonstrated in Figure 29. These results were\nobtained by finding the difference between the unaltered energy consumption and\nthe prediction energy consumption with the temperature increase and then\nmultiplying by the average cost of electricity in the United States. One aspect\nof global warming that often goes unmentioned is it will unevenly effect\ndifferent areas of the globe, with some locations enjoying longer growing\nseasons and others experiencing frequent severe weather events. The unequal\ndistribution of the effects of climate change are reflected in the figure which\ndemonstrates that according to the random forest model, some areas will see a\nreduction in energy usage (and hence cost) while other areas will see a\nsignificant increase in energy use. According to this analysis, an office\nbuilding of 50,000 ft2 could see an increase of nearly $30,000 in annual energy\ncosts due to increased temperatures. Many arguments have been made for taking\nsteps to reduce carbon emissions and transition to sustainable energy sources,\nbut perhaps none is more persuasive than that of simple economics.\n\nThe economic results of this experiment are presented below:\n\n```{r}\n# Economics of2 degree Celsius increase in temperature\nknitr::include_graphics('economics.png')\n```\n\nAdditional experiments are possible, modifying any of the weather or time variables.\nEventually, in order to quantify the effects of a building efficiency improvement,\na method will be developed to alter a building characteristic and gauge the effect.\nThis is an active area of research and work for the EDIFES team because putting\na precise figure on the savings from a recommendation is critical to the value\nof a virtual energy audit. The random forest experiment provides a framework\nfor how to alter one (or several if wanted) variables and observe the effects\non energy consumption by using the historical data.\n\n# Conclusion\n\nThe main question driving the EDA was \n\n__Which weather and time conditions are most correlated with energy use,__\n__and is it possible to build a model to predict energy use from the__\n__explanatory variables?__.\n\nThe query was addressed by a combination of data exploration, data visualization,\nand modeling. \nIn answer to the first half of the question, temperature, global horizontal \nirradiance, and diffuse horizontal irradiance are most highly positively \ncorrelated with energy consumption during the summer. In the winter, temperature\nis also highly correlated with energy consumption although the direction \nof this relationship varies across climate zones. In terms of time conditions, \nwhether or not the day is weekend is the most important variable followed\nby the cosine transformation of the time of day in hours, and then the raw \ntime of day in hours. \n\nIt is possible to predict energy use from the weather and time variables\nto a high degree of accuracy using machine learning methods. A random forest\nregression model was developed to predict six months worth of electricity\nconsumption and achieved near 80% accuracy averaged across all eight buildings.\nA complex, highly non-linear model was needed in order to capture all of the \npatterns and relationships within the data. Further work remains to be done\nwith the random forest model to improve performance, but it is a promising \nstart on the prediction problem. Additionally, the random forest can be used to \nperform experiments involving altering one or more variables and seeing the response\nin the context of historical data. Further models should be developed for the \nEDIFES project because relying on a single model is not smart choice, especially\nwith the ease of implementing many models using the R statistical language or\nPython and the numerous open-source libraries devoted to machine learning.\n\nOverall, the Progressive Building Energy EDA was an optimal introduction to the \nproblem domain of the EDIFES project. The author was able to gain an understanding\nof the structure of the data and the tools that will be used for virtual energy audits. \n\n# Acknowledgements\n\nThe author would like to thank the following members of the EDIFES team for their \nwillingness to explain all aspects of the project and for their tolerance of a significant \nnumber of questions: Professor Alexis Abramson, Professor Roger French, Rojiar Haddadian, \nArash Khalilnejad, Jack Mousseau, Shreyas Kamath, and Ahmad Karimi. \nThis project would not have been possible without their continuous support.\n\n# References\n\nFollowing is a nearly complete list of works used over the course of this semester. \nIt is impossible to document all the websites, stack overflow questions, \nbooks, and individuals consulted for this project, but acknowledge the help I \nreceived from each and every person contributing to this project.\n\n1.\tA. Abramson, R. French, J. Mousseau, A. Khalilnejad, M. Hossain, R. Haddadian, E. Pickering, W. Koehrsen, S. Kamath and K. Nash, BuildingRClean R Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/buildingrclean/overview\n2.\t\"About the Commercial Buildings Integration Program | Department of Energy\", Energy.gov, 2016. [Online]. Available: https://energy.gov/eere/buildings/about-commercial-buildings-integration-program. \n3.\tUniversity of Hawaii's Environmental Research and Design Laboratory, 2013, Energy Audit Procedures, US Department of Energy Office of Electricity Delivery and Energy Reliability. Available: https://www.hnei.hawaii.edu/sites/dev.hnei.hawaii.edu/files/Energy%20Audit%20Procedures.pdf\n4.\tEMS Environmental Incorporated, \"How Much Does a Commercial Energy Audit Cost\", 2017 [Online]. Available: http://emsenv.com/2016/04/28/commercial-energy-audit-cost/. \n5.\t\"ARPA-E | Virtual Building Energy Audits\", Arpa-e.energy.gov, 2015. [Online]. Available: https://arpa-e.energy.gov/?q=slick-sheet-project/virtual-building-energy-audits. \n6.\t\"GIS data and maps\", Solargis.com, 2017. [Online]. Available: http://solargis.com/products/maps-and-gis-data/. \n7.\tE. Pickering, \"EDIFES 0.4 Scalable Data Analytics for Commercial Building Virtual Energy Audits\", Masters of Science in Mechanical Engineering, Case Western Reserve University, 2017.\n8.\tM. Hossain, \"Development of Building Markers and an Unsupervised Non-Intrusive Disaggregation Model for Commercial Building Energy Usage\", Ph.D., Case Western Reserve University, Department of Mechanical and Aerospace Engineering, 2017.\n9.\tKim, H., Marwah, M., Arlitt, M., Lyon, G., and Han, J., 2011, \"Unsupervised Disaggregation of Low Frequency Power Measurements\", Proceedings of the 2011 SIAM International Conference on Data Mining, pp.747-758.Available http://epubs.siam.org/doi/abs/10.1137/1.9781611972818.64\n10.\tA. Zoha, A. Gluhak, M. A. Imran, and S. Rajasegarar, \"Non-Intrusive Load Monitoring Approaches for Disaggregated Energy Sensing: A Survey,\" Sensors, vol. 12, no. 12, pp. 16838-16866, Dec. 2012.\n11.\tD. Borthakur, \"HDFS Architecture Guide\", 2008 [Online]. Available: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html.\n12.\tSolar Degradation and Lifetime Extension (SDLE) Laboratory, cradletools Package. Cleveland, OH: Case Western Reserve University, 2017. Available: https://bitbucket.org/cwrusdle/cradletools/src\n13.\tG. K. Uyanik and N. G?ler, \"A Study on Multiple Linear Regression Analysis,\" Procedia - Social and Behavioral Sciences, vol. 106, no. Supplement C, pp. 234-240, Dec. 2013.\n14.\tB. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. Cambridge, MA, USA: MIT Press, 2001.\n15.\tDong, B., Cao, C., and Lee, S., 2005, \"Applying support vector machines to predict building energy consumption in tropical region\", Energy and Buildings, 37(5), pp. 545-553.Available: http://www.sciencedirect.com/science/article/pii/S0378778804002981\n16.\tA. S. Ahmad et al., \"A review on applications of ANN and SVM for building electrical energy consumption forecasting,\" Renewable and Sustainable Energy Reviews, vol. 33, no. Supplement C, pp. 102-109, May 2014.\n17.\tL. Breiman, \"Random Forests,\" Machine Learning, vol. 45, no. 1, pp. 5-32, Oct. 2001.\n18.\t\"World Maps of Koppen Geiger Climate Clssification\", Institute for Veterinary Pulic Health Vienna. 2017. [Online]. Availble: http://koeppen-geiger.vu-wien.ac.at/\n19.\tNordstrom, G. H. (2013). \"Using the Energy Signature Method to Estimate the Effective U Value of Buildings.\" Sustainability in Energy and Buildings Smart Innovation, Systems and Technologies, 35-44.\n20.\t\"Pearson Product-Moment Correlation\", Laerd Statistics 2017. [Online] Available: https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php\n21.\t\"Electric Power Monthly: Table 5.6., A Average Price of Electricity to Ultimate Customers by End-Use Sector\", United States Energy Information Administration, October 24, 2017. [Online]. Available: https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a\n22.\t\"Chicago Energy Benchmarking Homepage\", City of Chicago, 2017. [Online]. Available: https://www.cityofchicago.org/city/en/progs/env/building-energy-benchmarking---transparency.html\n23.\tR. Sakia, \"The Box-Cox Transformation Technique: A Review\", The Statistician, vol. 41, no. 2, p. 169, 1992. Available: http://www.jstor.org/stable/2348250?seq=1#page_scan_tab_contents\n24.\t\"scikit-learn: machine learning in Python\", Scikit-learn.org, 2017. [Online]. Available: http:/scikit-learn.org/stable/.\n25.\tJ. L. Lean and D. H. Rind, \"How will Earth's surface temperature change in future decades?,\" Geophys. Res. Lett., vol. 36, no. 15, p. L15708, Aug. 2009.\n26.\tS. R. Loarie, P. B. Duffy, H. Hamilton, G. P. Asner, C. B. Field, and D. D. Ackerly, \"The velocity of climate change,\" Nature, vol. 462, no. 7276, p. 1052, Dec. 2009.\n27.\tH. Zhao and F. Magouls, \"A review on the prediction of building energy consumption,\" Renewable and Sustainable Energy Reviews, vol. 16, no. 6, pp. 3586-3592, Aug. 2012.\n28.\tAmasyali, K., and El-Gohary, N., 2017, \"A review of data-driven building energy consumption prediction studies\", Renewable and Sustainable Energy Reviews, 81, pp. 1192-1205. Available: http://www.sciencedirect.com/science/article/pii/S1364032117306093\n29.\t\"Retrofitting Existing Buildings to Improve Sustainability and Energy Performance | WBDG Whole Building Design Guide.\" [Online]. Available:https:/www.wbdg.org/resources/retrofitting-existing-buildings-improve-sustainability-and-energy-performance. \n30.\tK. Cetin, M Siemann, C Sloop, \"Disaggregation and Future Prediction of Monthly Residential Building Energy Use Data Using Localized Weather Data Network,\" ACEEE Summer Study on Energy Efficiency in Buildings, 2016. Available: https://aceee.org/files/proceedings/2016/data/papers/12_410.pdf\n31.\tT. Hastie and R. Tibshirani, \"Generalized Additive Models,\" Statist. Sci., vol. 1, no. 3, pp. 314-318, Aug. 1986.\n32.\tQuick Start Guide to Prophet, CRAN, [Online]. Available: https://cran.r-project.org/web/packages/prophet/vignettes/quick_start.html.\n\n",
    "created" : 1512824752636.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "219479984",
    "id" : "6AE3060F",
    "lastKnownWriteTime" : 1512920711,
    "last_content_update" : 1512920711210,
    "path" : "~/DSCI 451/17f-dsci351-451-wjk68/1-assignments/SemProj-451/building_energy_eda/reports/eda_final_report.Rmd",
    "project_path" : "reports/eda_final_report.Rmd",
    "properties" : {
        "last_setup_crc32" : "559F29AFd0d15a42"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}